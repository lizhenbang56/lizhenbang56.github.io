<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="zhbli">
<meta property="og:url" content="http://yoursite.com/page/11/index.html">
<meta property="og:site_name" content="zhbli">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="zhbli">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/page/11/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>zhbli</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">zhbli</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-fw fa-calendar"></i>Schedule</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>Sitemap</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="fa fa-fw fa-heartbeat"></i>Commonweal 404</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/24/AAAI2020-SiamFC-Towards-Robust-and-Accurate-Visual-Tracking-with-Target-Estimation-Guidelines/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhbli">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="zhbli">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/24/AAAI2020-SiamFC-Towards-Robust-and-Accurate-Visual-Tracking-with-Target-Estimation-Guidelines/" class="post-title-link" itemprop="url">[AAAI2020] SiamFC++: Towards Robust and Accurate Visual Tracking with Target Estimation Guidelines</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-04-24 12:27:02" itemprop="dateCreated datePublished" datetime="2020-04-24T12:27:02+08:00">2020-04-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-05-13 20:40:52" itemprop="dateModified" datetime="2020-05-13T20:40:52+08:00">2020-05-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tracking/" itemprop="url" rel="index"><span itemprop="name">Tracking</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tracking/Architecture/" itemprop="url" rel="index"><span itemprop="name">Architecture</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>作者提出 4 条设计准则：</p>
<ul>
<li>decomposition of classification and state estimation。</li>
<li>classification score without ambiguity：在每个位置预测 1 个分类得分，并在该位置进行回归。<ul>
<li>基于 anchor 的机制为什么导致歧义？The anchor-based counterparts which consider the location on the input image as the center of multiple anchor boxes, output <strong>multiple classification score at the same location</strong> and regress the target bounding box with respect to these anchor boxes, leading to <strong>ambiguous</strong> matching between anchor and object. 同一个位置预测多个分类得分，这不好。</li>
</ul>
</li>
<li>tracking without prior knowledge：anchor 机制引入了先验，这是不好的。因此本文不基于 anchor 进行回归，而是直接进行回归。<ul>
<li>为什么 anchor 机制引入了先验？因为 anchor 指定了目标的尺度和长宽比。</li>
</ul>
</li>
<li>estimation quality score 质量得分：直接使用分类得分未必好。需要设计与分类得分无关的质量得分，作用是对预测的边框的质量进行打分。也就是说，对于一个位置，不仅能预测该位置有无目标，还能<strong>预测</strong>在该位置处的<strong>预测</strong>的边框质量。(<strong>predict</strong> the IoU score between <strong>predicted</strong> boxes and ground-truth boxes similar)</li>
</ul>
<p><img src="https://i.loli.net/2020/04/24/UqsEAreTQHgxkz8.png" alt="image-20200320203024927"></p>
<p>损失函数：</p>
<img src="https://i.loli.net/2020/04/24/zjLS4EfopyeaZ8W.png" alt="image-20200424123222296" style="zoom:50%;" />

<p>其中，$L_{cls}$ 是 focal loss，$L_{quality}$ 是用于 quality assessment 的 binary cross entropy (BCE) loss，$L_{reg}$ 是 IoU loss。注意，仅为正样本计算 $L_{quality}$ 和 $L_{reg}$。</p>
<p>本文中，使用 Prior Spatial Score (PSS) 计算 quality assessment：</p>
<img src="https://i.loli.net/2020/04/24/t9jlfEwTJnCBdQL.png" alt="image-20200424123921051" style="zoom:50%;" />
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/24/CVPR2019-Target-Aware-Deep-Tracking/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhbli">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="zhbli">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/24/CVPR2019-Target-Aware-Deep-Tracking/" class="post-title-link" itemprop="url">[CVPR2019] Target-Aware Deep Tracking</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-04-24 10:19:51" itemprop="dateCreated datePublished" datetime="2020-04-24T10:19:51+08:00">2020-04-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-05-13 20:47:02" itemprop="dateModified" datetime="2020-05-13T20:47:02+08:00">2020-05-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tracking/" itemprop="url" rel="index"><span itemprop="name">Tracking</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tracking/Feature-Extraction/" itemprop="url" rel="index"><span itemprop="name">Feature Extraction</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>现有跟踪算法的问题：很多跟踪算法使用在通用目标识别任务上预训练的模型，但预训练模型对跟踪任务的贡献不如对其他目标识别任务大。</p>
<p>原因：目标跟踪中的物体可能是任意类别。因此，预训练模型很难将目标与背景区分开。</p>
<p>本文的解决方案：学习 target-aware features，相比于预训练特征，可以更好地识别目标。为此，设计 regression loss 和 ranking loss，用于生成 target-active features 和 scale-sensitive features。我们根据反传梯度确定每个卷积滤波器的重要性，并基于用于表示目标的 activations 来选择 target aware features。Target-aware features 与孪生网络（siamfc）整合以进行目标跟踪。</p>
<p>性能：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://xinli-zn.github.io/TADT-project-page/">https://xinli-zn.github.io/TADT-project-page/</a></li>
<li>33.7 FPS</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>使用预训练模型表示目标特征时，存在很多问题：</p>
<ol>
<li>目标可能是任意形式的。比如，目标可能是训练集中未见过的物体。或者，可能仅仅是物体的一部分。</li>
<li>来自最后一个卷积的特征往往仅具有高层视觉信息，缺乏精确定位和尺度估计信息。</li>
<li>分类网络致力于缩小类内差距，用于跟踪时，无法区分同类物体。</li>
<li>计算量大。</li>
</ol>
<p>一个分类网络中，反向传播的梯度可以反应特定类别的显著性。</p>
<blockquote>
<p>[33] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. <strong>Grad-cam: Visual explanations from deep networks via gradient-based localization</strong>. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.</p>
</blockquote>
<p>使用 global average pooling，卷积滤波器的梯度能够决定用于表示该滤波器表示一个目标的重要性。为了选择最有效的滤波器，我们设计了两种损失：</p>
<ol>
<li>使用 hinge loss 将预训练特征回归到由高斯函数生成的 soft labels，并使用梯度选择 target-active 卷积滤波器。</li>
<li>使用 ranking loss with pair-wise distance 寻找 scale-aware 卷积滤波器。</li>
</ol>
<p>所选的重要滤波器的 activations 便是本文的 target-aware features。由于仅使用部分滤波器，计算量也减少了。</p>
<h2 id="Target-Aware-Features"><a href="#Target-Aware-Features" class="headerlink" title="Target-Aware Features"></a>Target-Aware Features</h2><img src="https://i.loli.net/2020/04/24/rLuQx19GUcgDTas.png" alt="image-20200424102819639" style="zoom:50%;" />

<p>我们首先分析来自预训练分类模型的特征与用于跟踪的有效表示之间的差距。然后提出 target-aware feature model。</p>
<h3 id="Features-of-pre-trained-CNNs"><a href="#Features-of-pre-trained-CNNs" class="headerlink" title="Features of pre-trained CNNs"></a>Features of pre-trained CNNs</h3><p>给定输出特征空间为 $\mathcal{X}$ 预训练特征提取器，可以跟踪通道重要性 $\Delta$ 生成子空间 $\mathcal{X}’$：</p>
<img src="https://i.loli.net/2020/04/24/LNPET1DhOlfpS9W.png" alt="image-20200424103046271" style="zoom:50%;" />

<p>其中 $\varphi$ 是用于选择重要通道的 mapping function。通道 $i$ 的重要性 $\Delta_i$ 计算为：</p>
<img src="https://i.loli.net/2020/04/24/FTfJ5xceyjULkPV.png" alt="image-20200424103125966" style="zoom:50%;" />

<p>其中 $G_{AP}(\cdot)$ 指 global average pooling 函数，$L$ 是设计的损失，$z_i$ 是第 $i$ 个滤波器的输出特征。</p>
<h3 id="Target-Active-Features-via-Regression"><a href="#Target-Active-Features-via-Regression" class="headerlink" title="Target-Active Features via Regression"></a>Target-Active Features via Regression</h3><p>将与目标中心对齐的图像块中的所有样本 $X_{i,j}$ 回归到高斯 label map $Y(i,j) = e^{-\frac{i^2+j^2}{2\sigma ^2}}$：</p>
<img src="https://i.loli.net/2020/04/24/hgXBDW8LCvrFmqf.png" alt="image-20200424103220532" style="zoom:50%;" />

<p>其中 $W$ 是 regressor weight。可以通过每个滤波器对拟合 label map 的贡献，计算其重要性：</p>
<img src="https://i.loli.net/2020/04/24/n5X4GI9cs3OUJ6t.png" alt="image-20200424103239827" style="zoom:50%;" />

<p>其中 $X_o$ 是 output prediction。</p>
<h3 id="Scale-Sensitive-Features-via-Ranking"><a href="#Scale-Sensitive-Features-via-Ranking" class="headerlink" title="Scale-Sensitive Features via Ranking"></a>Scale-Sensitive Features via Ranking</h3><p>我们需要寻找对尺度变化最敏感的滤波器。我们将该问题建模为 ranking model：对训练样本按照尺寸接近目标的程度进行排序。我们利用了 [23] 中的 smooth approximated ranking loss 实现这一点：</p>
<blockquote>
<p>[23] Yuncheng Li, Yale Song, and Jiebo Luo. Improving pairwise ranking for multi-label image classification. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.</p>
</blockquote>
<img src="https://i.loli.net/2020/04/24/QI8a4lqNt1ijvnP.png" alt="image-20200424103420326" style="zoom:50%;" />

<p>其中 $(x_i, x_j)$ 是成对训练样本，$x_j$ 相比于 $x_i$，于目标尺寸的差距更小。$f(x,w)$ 是预测模型。</p>
<p>梯度计算为：</p>
<img src="https://i.loli.net/2020/04/24/We8yw3VgiETcJn2.png" alt="image-20200424103636096" style="zoom:50%;" />

<img src="https://i.loli.net/2020/04/24/S2xQhvVzaYIibWk.png" alt="image-20200424103655075" style="zoom:50%;" />

<p>其中 $W$ 是卷积层中的滤波器权重。</p>
<h2 id="Tracking-Process"><a href="#Tracking-Process" class="headerlink" title="Tracking Process"></a>Tracking Process</h2><p>提出的跟踪框架由如下部分组成：</p>
<ol>
<li>pre-trained feature extractor：使用分类任务离线训练。</li>
<li>target-aware feature module：仅使用第一帧训练。</li>
<li>Siamese matching module</li>
</ol>
<p>测试时，使用 target-aware features 计算相似性得分：</p>
<img src="https://i.loli.net/2020/04/24/rwXMPSqLpWRAoaE.png" alt="image-20200424103908919" style="zoom:50%;" />
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/22/CVPR2018-Learning-to-Compare-Relation-Network-for-Few-Shot-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhbli">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="zhbli">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/22/CVPR2018-Learning-to-Compare-Relation-Network-for-Few-Shot-Learning/" class="post-title-link" itemprop="url">[CVPR2018] Learning to Compare: Relation Network for Few-Shot Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-04-22 15:04:56 / Modified: 15:08:16" itemprop="dateCreated datePublished" datetime="2020-04-22T15:04:56+08:00">2020-04-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>提出 Relation Network 用于 few-shot learning，其中分类器必须学习如何在每个类别仅提供几个样本的情况下识别新的类别。通过元学习，网络学习如何学习</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/22/CVPR2020-Circle-Loss-A-Unified-Perspective-of-Pair-Similarity-Optimization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhbli">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="zhbli">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/22/CVPR2020-Circle-Loss-A-Unified-Perspective-of-Pair-Similarity-Optimization/" class="post-title-link" itemprop="url">[CVPR2020] Circle Loss: A Unified Perspective of Pair Similarity Optimization</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-04-22 12:52:22 / Modified: 14:05:50" itemprop="dateCreated datePublished" datetime="2020-04-22T12:52:22+08:00">2020-04-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>本文针对深度特征学习提出了两个相似性优化视角，旨在最大化类内相似度 $s_p$ 并最小化类间相似度 $s_n$。</p>
<p>我们发现大多数损失函数，包括 triplet loss 和 softmax+cross-entropy loss，将 $s_n$ 和 $s_p$ 嵌入到 similarity pairs 并减少 $(s_n-s_p)$。这种优化方式是不灵活的，因为对每个相似度得分的惩罚被限制为相等。</p>
<p>我们的直觉是，如果相似性得分距离最优值很远，则应予以强调。为此，我们只需对每个相似度重新加权，以强调未优化的相似度得分。这就是 circle loss，因为决策边界是圆形的。</p>
<p>circle loss 为两种基本的深度特征学习方法构建了统一的公式：</p>
<ul>
<li>learning with class-level labels</li>
<li>learning with pair-wise labels</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>无论是 triplet loss 还是 softmax+cross-entropy loss，都致力于减少 $(s_n-s_p)$。在 $(s_n-s_p)$ 中，增加 $s_p$ 相当于减少 $s_n$。我们认为这种对称优化方式容易导致以下两个问题：</p>
<ol>
<li>缺乏优化灵活性。对 $s_n$ 和 $s_p$ 的惩罚力度是相等的，它们的梯度幅度相同。特殊情况下，如 $s_p$ 很小，而 $s_n$ 早已是 0，此时仍然会以大的梯度惩罚 $s_n$。这是低效且不合理的。</li>
<li>收敛状态不明确。优化 $(s_n-s_p)$ 通常导致决策边界 $s_n-s_p = m$，$m$ 是 margin。</li>
</ol>
<p>我们认为，不同的相似性得分应该具有不同的惩罚力度。如果一个相似性得分远未被优化，应该具有更强的惩罚。反之亦然。因此我们将 $(s_n-s_p)$ 推广为 $(\alpha_n s_n-\alpha_p s_p)$。这导致优化边界是 $(\alpha_n s_n-\alpha_p s_p) = m$，这是在 $(s_n-s_p)$ 空间种的一个圆，因此称作 circle loss。</p>
<p>虽然很简单，circle loss 从本质上重塑了深度特征学习，体现在如下方面：</p>
<ol>
<li>统一的损失函数：我们为两种基本的学习方法（使用 class-level labels 和 pair-wise labels）提出了统一的损失函数。</li>
<li>灵活的优化：不同的相似性得分具有不同的惩罚力度。</li>
<li>确定的收敛状态：确立了明确的优化目标并有利于可分性。</li>
</ol>
<h2 id="A-Unified-Perspective"><a href="#A-Unified-Perspective" class="headerlink" title="A Unified Perspective"></a>A Unified Perspective</h2><p>在 cosine 相似性度量下，我们希望 $s_p \rightarrow 1$，$s_n \rightarrow 0$。</p>
<p>给定 class-level labels，常用的方法有：L2-Softmax，Large-margin Softmax，Angular Softmax，NormFace，AM-Softmax，CosFace，ArcFace。</p>
<p>给定 pair-wise labels，常用的方法有：constrastive loss，triplet loss，Lifted-Structure loss，N-pair loss，Histogram loss，Angular loss，Margin based loss，Multi-Similarity loss。</p>
<p>给定特征空间的一个样本 $x$，假设有 $K$ 个类内相似度得分和 $L$ 个类间相似度得分，分别定义为 ${s^i_p}(i=1,2,…,K)$ 和 ${s^j_n}(j=1,2,…,L)$。</p>
<p>为了最小化每个 $s^j_n$ 同时最大化每个 $s^i_p$，我们提出了统一的损失函数：<img src="https://i.loli.net/2020/04/22/Js9CdyA5hfRU8ic.png" alt="image-20200422135329596" style="zoom:50%;" /></p>
<p>该函数通过简单修改就能退化成 triplet loss 或 classification loss。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/21/arXiv2001-Deep-Learning-for-Person-Re-identification-A-Survey-and-Outlook/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhbli">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="zhbli">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/21/arXiv2001-Deep-Learning-for-Person-Re-identification-A-Survey-and-Outlook/" class="post-title-link" itemprop="url">[arXiv2001] Deep Learning for Person Re-identification: A Survey and Outlook</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-04-21 19:17:48 / Modified: 19:44:00" itemprop="dateCreated datePublished" datetime="2020-04-21T19:17:48+08:00">2020-04-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><img src="https://i.loli.net/2020/04/21/o2fRQlx6d1873vz.png" alt="image-20200421192713520"></p>
<p>Identity Loss：将 reid 视为图像分类问题。</p>
<p>Verification Loss：用于优化成对关系。包括 contrastive loss 和 binary verification loss。</p>
<ul>
<li><p>contrastive loss：</p>
<img src="https://i.loli.net/2020/04/21/G9ERy8rfFTYegNA.png" alt="image-20200421193745441" style="zoom:50%;" />
</li>
<li><p>verification loss with cross-entropy：</p>
<img src="https://i.loli.net/2020/04/21/F6laoOyuEPsvepT.png" alt="image-20200421193902135" style="zoom:50%;" />
</li>
<li><p>Triplet loss</p>
<img src="https://i.loli.net/2020/04/21/nCGsM1UQLdZyhmP.png" alt="image-20200421194054482" style="zoom:50%;" /></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/21/arXiv1703-In-Defense-of-the-Triplet-Loss-for-Person-Re-Identification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhbli">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="zhbli">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/21/arXiv1703-In-Defense-of-the-Triplet-Loss-for-Person-Re-Identification/" class="post-title-link" itemprop="url">[arXiv1703] In Defense of the Triplet Loss for Person Re-Identification</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-04-21 19:03:31 / Modified: 19:04:40" itemprop="dateCreated datePublished" datetime="2020-04-21T19:03:31+08:00">2020-04-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>在 reid 领域，目前认为 triplet loss 不如其他损失（classification loss，verification loss）+ 单独的度量学习性能好。本文证明使用 triplet 损失端到端训练的度量学习比其他方法都好。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>classification loss 的缺点：训练参数随类别数而增加，而这些参数在训练后被丢弃。</p>
<p>verification loss 的缺点：仅在 cross-image representation 模式下可用，即仅能回答“这两幅图有多像”这一问题。这使得在其他任务如 clustering 和 retrieval 时难以应用，因为每个 probe 必须与所有 gallery image 配对。</p>
<p>triplet loss 不受欢迎的原因：如果简单应用，则性能不佳。一个要点是难例挖掘，否则训练将止步不前。然而很难明确定义难例，同时很费时。另外，使用太难的 triplet loss 容易使训练不稳定。</p>
<p>本文设计了基于 batch hard sample mining 的 triplet loss，改善了性能。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/21/arXiv1611-A-Discriminatively-Learned-CNN-Embedding-for-Person-Re-identification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhbli">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="zhbli">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/21/arXiv1611-A-Discriminatively-Learned-CNN-Embedding-for-Person-Re-identification/" class="post-title-link" itemprop="url">[arXiv1611] A Discriminatively Learned CNN Embedding for Person123 Re-identification</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-04-21 17:59:11" itemprop="dateCreated datePublished" datetime="2020-04-21T17:59:11+08:00">2020-04-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-22 10:51:18" itemprop="dateModified" datetime="2020-04-22T10:51:18+08:00">2020-04-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>本文回顾了两种 reid 网络结构：verification model 和 identification model。这两种模型各有优缺点。本文结合了这两种模型：提出孪生网络同时计算 identification loss 和 verification loss。给定一个图像对，网络预测两幅图像的身份，同时预测它们是否同一身份。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Verification model 的问题：仅使用 weak reid labels，没有充分利用标签信息，因此没有考虑图像对与数据集中其他图像之间的关系。</p>
<p>为了充分利用 reid 标签，identification models 将 reid 视为多类识别问题。缺点是训练任务与测试不完全一致。</p>
<img src="https://i.loli.net/2020/04/22/Z3jKr42lFUHna9q.png" alt="image-20200422104418316" style="zoom:50%;" />

<img src="https://i.loli.net/2020/04/22/8heMbH7zpnFB94N.png" alt="image-20200422104845255" style="zoom:50%;" />
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/21/CVPR2019-Joint-Discriminative-and-Generative-Learning-for-Person-Re-identification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhbli">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="zhbli">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/21/CVPR2019-Joint-Discriminative-and-Generative-Learning-for-Person-Re-identification/" class="post-title-link" itemprop="url">[CVPR2019] Joint Discriminative and Generative Learning for Person Re-identification</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-04-21 15:57:21 / Modified: 18:58:52" itemprop="dateCreated datePublished" datetime="2020-04-21T15:57:21+08:00">2020-04-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>首次提出，将 reid 学习和数据生成整合到一个端到端框架中，名为 GD-Net。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/NVlabs/DG-Net">https://github.com/NVlabs/DG-Net</a></p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="Generative-Module"><a href="#Generative-Module" class="headerlink" title="Generative Module"></a>Generative Module</h3><p>generative module：</p>
<ul>
<li>输入：训练集中的两幅真实图像。其中一幅真实图像用于提供 appearance code，另一幅真实图像用于提供 structure code。structure code 的空间分辨率更高，以保留几何和位置属性。</li>
<li>输出：新的行人图像。</li>
<li>结构<ul>
<li>appearance encoder $E_a:x_i \rightarrow a_i$</li>
<li>structure encoder $E_s:x_j \rightarrow s_j$</li>
<li>decoder $G: (a_i,s_j)\rightarrow x_j^i$</li>
<li>discriminator $D$：用于区分图像是生成的还是真实的。</li>
</ul>
</li>
<li>objectives<ul>
<li>self-identity generation，用于规范 generator。</li>
<li>cross-identity generation，是生成的图像可控并匹配帧数数据分布。</li>
</ul>
</li>
</ul>
<h4 id="Self-identity-generation"><a href="#Self-identity-generation" class="headerlink" title="Self-identity generation"></a>Self-identity generation</h4><p>给定个一幅图像，generative module 首先学习如何重构这幅输入图像。这在整个生成过程中起到了重要的正则化作用。同身份、同图像重构损失如下：</p>
<img src="https://i.loli.net/2020/04/21/2GyKsJvF8EXZm3w.png" alt="image-20200421162403905" style="zoom:50%;" />

<p>我们还假设对于同一类别的两个行人图像 $x_i,x_t$，使用 $x_i$ 的结构信息以及 $x_t$ 的表观信息就能重构 $x_i$ 的表观信息。同身份、跨图像重构损失如下：</p>
<img src="https://i.loli.net/2020/04/21/D9BnNU4V7iqwvXz.png" alt="image-20200421162704711" style="zoom:50%;" />

<p>为了保证不同图像的 appearance code 不同，使用 identification loss 区分不同身份：</p>
<img src="https://i.loli.net/2020/04/21/g5yzAWNhDrcbJQs.png" alt="image-20200421163100753" style="zoom:50%;" />

<p>其中 $p(y_i|x_i)$ 是基于 appearance code 预测的 $x_i$ 输入 ground truth class $y_i$ 的概率。</p>
<h4 id="Cross-identity-generation"><a href="#Cross-identity-generation" class="headerlink" title="Cross-identity generation"></a>Cross-identity generation</h4><p>使用不同身份生成图像。因此没有 pixel-level 的监督信号。相反，我们引入 latent code reconstruction，基于 appearance code 和 structure code 来控制图像生成。我们希望生成的图像 $x^i_j=G(a_i,s_j)$ 具有 $x_i$ 的表观和 $x_j$ 的结构。因此我们在得到生成图像后重构 appearance code 和 structure code：</p>
<img src="https://i.loli.net/2020/04/21/QiHmtA2DjERqfPT.png" alt="image-20200421163855378" style="zoom:50%;" />

<p>此外，为了保持身份一致性，使用生成图像的 appearance code 计算 identification loss：</p>
<img src="https://i.loli.net/2020/04/21/Kl4vXuZLVkbpm7c.png" alt="image-20200421164219836" style="zoom:50%;" />

<p>另外，我们还用对抗损失使得生成图像与真实图像的分布相同：</p>
<img src="https://i.loli.net/2020/04/21/Wx1mhf4gBctA5Ly.png" alt="image-20200421164342052" style="zoom:50%;" />

<h3 id="Discriminative-Module"><a href="#Discriminative-Module" class="headerlink" title="Discriminative Module"></a>Discriminative Module</h3><h4 id="Primary-feature-learning"><a href="#Primary-feature-learning" class="headerlink" title="Primary feature learning"></a>Primary feature learning</h4><p>我们最小化如下两者的 KL 散度：</p>
<ol>
<li>由 discriminative module 预测的概率分布 $p(x^i_j)$</li>
<li>由 teacher 预测的概率分布 $q(x^i_j)$</li>
</ol>
<img src="https://i.loli.net/2020/04/21/LKe1QjYX3HcyTaV.png" alt="image-20200421165125391" style="zoom:50%;" />

<h3 id="Fine-grained-feature-mining"><a href="#Fine-grained-feature-mining" class="headerlink" title="Fine-grained feature mining"></a>Fine-grained feature mining</h3><img src="https://i.loli.net/2020/04/21/PmBgW2JFAvQux9K.png" alt="image-20200421165610583" style="zoom:50%;" />
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/21/CVPR2019-ArcFace-Additive-Angular-Margin-Loss-for-Deep-Face-Recognition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhbli">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="zhbli">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/21/CVPR2019-ArcFace-Additive-Angular-Margin-Loss-for-Deep-Face-Recognition/" class="post-title-link" itemprop="url">[CVPR2019] ArcFace: Additive Angular Margin Loss for Deep Face Recognition</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-04-21 13:45:14 / Modified: 15:35:46" itemprop="dateCreated datePublished" datetime="2020-04-21T13:45:14+08:00">2020-04-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Centre loss 为实现类内紧凑型，在欧氏空间中惩罚特征到类中心的距离。</p>
<p>SphereFase 假定最后一个全连接层中的线性变换矩阵可以被作用角度空间中类中心的表示。因此以 multiplicative 方式惩罚特征和对应权重的角度。</p>
<p>最近，为了最大化人脸类别的可分离性，将 margins 整合到以后的损失中。</p>
<p>本文提出 Additive Angular Margin Loss（ArcFace）来获得高度判别行的特征。ArcFace 具有清晰的几何解释，因为它与超球上的测地距离具有确切的对应。</p>
<p>本算法取得了最好的性能，同时增加的计算量几乎可以忽略不计。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/deepinsight/insightface">https://github.com/deepinsight/insightface</a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>主要有两种方法训练人脸识别网络：</p>
<ol>
<li><p>使用 softmax 训练多类分类器。</p>
<ul>
<li>softmax loss 的缺点：<ol>
<li>线性变化矩阵的尺寸随类别数而增加。</li>
<li>难以适用于开集人脸识别任务。</li>
</ol>
</li>
<li>改进：<ul>
<li>centre loss（ECCV2016）。缺点：训练时更新实际的中心很困难，因为人脸类别很多。</li>
<li>Sphereface（CVPR2017）。缺点：损失函数需要一系列近似，导致训练不稳定。为了稳定训练，与标准 softmax 混合训练。然而这又容易使 softmax 损失主导训练过程。</li>
<li>CosFace（CVPR2018）。</li>
</ul>
</li>
</ul>
</li>
<li><p>直接学习 embedding（利用，使用 triplet loss）。triplet loss 的缺点：</p>
<ol>
<li>大规模数据集中三元组数量会组合爆炸，导致更多的迭代次数。</li>
<li>semi-hard sample mining 很困难。</li>
</ol>
</li>
</ol>
<p>本文方法的优点总结如下：</p>
<ul>
<li>Engaging：通过归一化超球面中角度和弧度之间的精确对应关系，直接优化了 geodesic distance margin。</li>
<li>Effective：性能最优。</li>
<li>Easy：仅需几行代码。不需要与其他损失函数整合以稳定训练，在任何训练集上都易于训练。</li>
<li>Efficient：计算量很小，可以轻松对数百万个身份进行训练。</li>
</ul>
<h2 id="Proposed-Approach"><a href="#Proposed-Approach" class="headerlink" title="Proposed Approach"></a>Proposed Approach</h2><h3 id="ArcFace"><a href="#ArcFace" class="headerlink" title="ArcFace"></a>ArcFace</h3><p>最常用的 softmax 分类损失定义如下：</p>
<img src="https://i.loli.net/2020/04/21/M4UOceLat8jmFzA.png" alt="image-20200421151700044" style="zoom:50%;" />

<p>其中，特征维度是 512。</p>
<p>softmax 的缺点：未明确优化 embedding，以使得类内样本更相似，类间样本更多样。从而导致当类内表观变化大或者测试集很大时性能不佳。</p>
<p>接下来对损失改进，使得学得的特征分布在半径为 $s$ 的超球上：</p>
<img src="https://i.loli.net/2020/04/21/XfRs9zDEavAlSQo.png" alt="image-20200421151725782" style="zoom:50%;" />

<p>进一步，我们通过添加 margin penalty 改善类内的紧凑性和类间的可分性。</p>
<img src="https://i.loli.net/2020/04/21/qDaz3XLM9PTVIRY.png" alt="image-20200421151857337" style="zoom:50%;" />

<h3 id="Comparison-with-SphereFace-and-CosFace"><a href="#Comparison-with-SphereFace-and-CosFace" class="headerlink" title="Comparison with SphereFace and CosFace"></a>Comparison with SphereFace and CosFace</h3><p>margin penalty 不同：</p>
<ul>
<li>multiplicative angular margin：SphereFace</li>
<li>additive angular margin：ArcFace</li>
<li>additive cosine margin：CosFase</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/21/CVPR2020-Data-Uncertainty-Learning-in-Face-Recognition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhbli">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="zhbli">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/21/CVPR2020-Data-Uncertainty-Learning-in-Face-Recognition/" class="post-title-link" itemprop="url">[CVPR2020] Data Uncertainty Learning in Face Recognition</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-04-21 10:16:21 / Modified: 13:22:54" itemprop="dateCreated datePublished" datetime="2020-04-21T10:16:21+08:00">2020-04-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>现有人脸识别算法的问题：对于 noisy images，建模数据不确定性是重要的，但是在人脸识别中很少被研究。先前工作 [35] 把每个人脸嵌入建模为高斯分布从而考虑不确定性。然而它使用来自现有模型的固定特征（高斯的均值）。这仅估计了方差，并依赖与专门设计的、计算量高的度量方法，因此不易使用。目前仍不清楚不确定性如何影响特征学习。</p>
<blockquote>
<p>Yichun Shi, Anil K Jain, and Nathan D Kalka. <strong>Probabilistic face embeddings</strong>. In Proceedings of the IEEE International Conference on Computer Vision, 2019.</p>
</blockquote>
<p>本文将数据不确定性学习应用于人脸识别，首次使得特征（均值）和不确定性（方差）可以同时学习。提出两种学习方法，易于使用且性能良好。</p>
<p>我们还分析了整合不确定性估计如何帮助减少噪声样本的负面影响，以及如何影响特征学习的。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>数据不确定性捕获数据的固有噪声。建模这种不确定性是重要的，因为噪声广泛存在于图像中。</p>
<p>大多数人脸识别方法将每张人脸图像表示为隐空间中一个确定性的嵌入点。通常，高质量的图像中，相同 ID 的人脸图像的特征是会聚集在一起的。然而很难为有噪声的人脸图像估计准确的嵌入点，通常位于 cluster 之外，并在嵌入空间中有很大的不确定性。</p>
<p>Probabilistic face embeddings（PFE）[35] 是首先考虑人脸识别中数据不确定性的工作。 对于每个样本，在隐空间中估计高斯分布，而不是一个固定点。具体而言，给定预训练的 FR 模型，每个样本的高斯均值固定为 FR 模型产生的嵌入。在 FR 模型添加并训练额外的分支以估计方差。训练通过新的相似性度量来进行：mutual likelihood score（MLS），用于度量两个高斯分布的 likelihood。PFE 为高质量的样本估计小的方差，为噪声样本估计大的方差。因此 PFE 可以减低噪声样本的错误匹配。</p>
<p>然而，PFE 仅学习不确定性，未学习嵌入特征（mean），因此不知道不确定性如何影响特征学习。同时传统的相似性度量如 cosine 距离无法使用。需要更复杂的 MLS 度量，增加了运行时间和内存。</p>
<p>我们首次将数据不确定性学习（DUL）引入人脸识别，使得特征（均值）和不确定性（方差）可以同时学习。这改善了特征，使得同类特征更紧凑。学习的特征可以直接使用传统相似性度量，不再需要 MLS 度量。</p>
<p>具体而言，我们提出两个学习方法：</p>
<ol>
<li>第一个方法是基于分类的，从头学习一个一个模型。</li>
<li>第二个方法是基于回归的，用于改善现有模型。</li>
</ol>
<p>我们从图像噪声的角度，讨论了学习的不确定性是如何影响这两种方法的模型训练的：学习的不确定性通过自适应降低噪声训练样本的负面影响，来改善特征嵌入的学习。</p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>很多方法引入的 deep uncertainty learning，用于改进模型的鲁棒性和可解释性：</p>
<ul>
<li>语义分割<ul>
<li>Shuya Isobe and Shuichi Arai. <strong>Deep convolutional encoderdecoder network with model uncertainty for semantic segmentation</strong>. In 2017 IEEE International Conference on INnovations in Intelligent SysTems and Applications (INISTA), pages 365–370. IEEE, 2017.</li>
<li>Alex Kendall, Vijay Badrinarayanan, and Roberto Cipolla. <strong>Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding</strong>. BMVC, 2015.</li>
</ul>
</li>
<li>目标检测<ul>
<li>Jiwoong Choi, Dayoung Chun, Hyun Kim, and Hyuk-Jae Lee. <strong>Gaussian yolov3: An accurate and fast object detector using localization uncertainty for autonomous driving</strong>. In The IEEE International Conference on Computer Vision (ICCV), October 2019.</li>
<li>Florian Kraus and Klaus Dietmayer. <strong>Uncertainty estimation in one-stage object detection</strong>. arXiv preprint arXiv:1905.10296, 2019.</li>
</ul>
</li>
<li>Re-ID<ul>
<li>Tianyuan Yu, Da Li, Yongxin Yang, Timothy M Hospedales, and Tao Xiang. <strong>Robust person re-identification by modelling feature uncertainty</strong>. In Proceedings of the IEEE International Conference on Computer Vision, pages 552–561, 2019.</li>
</ul>
</li>
</ul>
<p>人脸识别中，有对模型不确定性和数据不确定性的研究：</p>
<ul>
<li>模型不确定性<ul>
<li>Sixue Gong, Vishnu Naresh Boddeti, and Anil K Jain. <strong>On the capacity of face representation</strong>. arXiv preprint arXiv:1709.10433, 2017.</li>
<li>Umara Zafar, Mubeen Ghafoor, Tehseen Zia, Ghufran Ahmed, Ahsan Latif, Kaleem Razzaq Malik, and Abdullahi Mohamud Sharif. <strong>Face recognition with bayesian convolutional networks for robust surveillance systems</strong>. EURASIP Journal on Image and Video Processing, 2019(1):10, 2019.</li>
<li>Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. <strong>Striking the right balance with uncertainty</strong>. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 103–112, 2019.</li>
</ul>
</li>
<li>数据不确定性<ul>
<li>Yichun Shi, Anil K Jain, and Nathan D Kalka. <strong>Probabilistic face embeddings</strong>. In Proceedings of the IEEE International Conference on Computer Vision, 2019.</li>
</ul>
</li>
</ul>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><h3 id="Classification-based-DUL-for-FR"><a href="#Classification-based-DUL-for-FR" class="headerlink" title="Classification-based DUL for FR"></a>Classification-based DUL for FR</h3><h4 id="Distributional-Representation"><a href="#Distributional-Representation" class="headerlink" title="Distributional Representation"></a>Distributional Representation</h4><p>将每个样本 $\mathbf{x}_i$ 在隐空间中的特征表示 $\mathbf{z}_i$ 定义为高斯分布：</p>
<img src="https://i.loli.net/2020/04/21/HcBsDzOleWUp5N3.png" alt="image-20200421101935936" style="zoom:50%;" />

<p>其中，高斯分布的均值和方差都与输入相关，并通过 CNN 预测：</p>
<img src="https://i.loli.net/2020/04/21/MjtHph4kBedfzQC.png" alt="image-20200421111911376" style="zoom:50%;" />

<p>此处预测的高斯是 diagonal multivariate normal。$\pmb{\mu}_i$ 可被视作人脸的理想特征，$\pmb{\sigma}_i$ 可视作不确定性。现在，每个样本的表示不再是固定的嵌入点，而是从高斯分布中随机采样的嵌入。</p>
<p>然而采样操作是无法反向传播的。我们使用 re-parameterization trick [24] 使得模型仍然能计算梯度。具体而言，首先从正太分布中采样与模型参数无关的随机噪声 $\epsilon$，然后生成 $\mathbf{s}_i$ 作为等效的特征表示（公式2）：</p>
<img src="https://i.loli.net/2020/04/21/9VK7SxLcGrEw1Z5.png" alt="image-20200421101958743" style="zoom:50%;" />

<p>其中，$\mathbf{s}_i$ 是图像的最终表示。</p>
<h4 id="Classification-Loss"><a href="#Classification-Loss" class="headerlink" title="Classification Loss"></a>Classification Loss</h4><p>使用分类器最小化 softmax 损失（公式3）：</p>
<img src="https://i.loli.net/2020/04/21/gPpFnJNcm14ojk6.png" alt="image-20200421102125834" style="zoom:50%;" />

<p>实际上，使用 softmax 损失的不同变种来训练分类模型：</p>
<ul>
<li>additive margin：Feng Wang, Jian Cheng, Weiyang Liu, and Haijun Liu. <strong>Additive margin softmax for face verification</strong>. IEEE Signal Processing Letters, 25(7):926–930, 2018.</li>
<li>feature $\ell2$ normalization： Rajeev Ranjan, Carlos D Castillo, and Rama Chellappa. <strong>L2-constrained softmax loss for discriminative face verification</strong>. arXiv preprint arXiv:1703.09507, 2017.</li>
<li>arcface：Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. <strong>Arcface: Additive angular margin loss for deep face recognition</strong>. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4690– 4699, 2019.</li>
</ul>
<h4 id="KL-Divergence-Regularization"><a href="#KL-Divergence-Regularization" class="headerlink" title="KL-Divergence Regularization"></a>KL-Divergence Regularization</h4><p>公式 2 表明，在训练时所有特征嵌入都会被 $\pmb{\sigma}_i$ 破坏，这会让网络对所有样本都预测小的 $\pmb \mu_i$ 来抑制 $\mathbf{s}_i$ 的不稳定成分，这样公式 3 仍然能收敛，不过退退化成原始的确定性表示。</p>
<p>受 variational information bottleneck [1] 的启发，在优化时引入正则项：显式约束 $\mathcal{N}(\pmb \mu_i, \pmb \sigma_i)$ 为正态分布，者通过 KL 散度来度量：</p>
<img src="https://i.loli.net/2020/04/21/AWohfxjNg9DPHSX.png" alt="image-20200421102221253" style="zoom:50%;" />

<blockquote>
<p>Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. <strong>Deep variational information bottleneck</strong>. In Proceedings of the International Conference on Learning Representations, 2017.</p>
</blockquote>
<h3 id="Regression-based-DUL-for-FR"><a href="#Regression-based-DUL-for-FR" class="headerlink" title="Regression-based DUL for FR"></a>Regression-based DUL for FR</h3><h4 id="Difficulty-of-Introducing-Data-Uncertainty-Regression-to-FR"><a href="#Difficulty-of-Introducing-Data-Uncertainty-Regression-to-FR" class="headerlink" title="Difficulty of Introducing Data Uncertainty Regression to FR"></a>Difficulty of Introducing Data Uncertainty Regression to FR</h4><p>由于在人脸的映射空间 $\mathcal{X \rightarrow Y}$ 中，$\mathcal{X}$ 是连续的，但 $\mathcal{Y}$ 是离散的，因此不能直接应用数据不确定性回归。</p>
<h4 id="Constructing-New-Mapping-Space-for-FR"><a href="#Constructing-New-Mapping-Space-for-FR" class="headerlink" title="Constructing New Mapping Space for FR"></a>Constructing New Mapping Space for FR</h4><p>我们为人脸数据构建了连续的 target space，这与原始的离散 target space 几乎等价。步骤如下：</p>
<ol>
<li>预训练基于分类的确定性 FR 模型。</li>
<li>利用分类模型的分类层 $\mathcal W \in \mathbb R^{D\times C}$ 作为 expected target vector。其中 $D$ 是嵌入的维度，$C$ 是训练集的类别数。</li>
<li>由于每个 $\mathbf w_i \in \mathcal W$ 可以看作具有相同类别的嵌入的 typical center，${\mathcal{X,W}}$ 可以看作新的 equivalent mapping space。${\mathcal{X,W}}$ 同样具有固有噪声。</li>
<li>我们可以建立从 $\mathbf x_i \in \mathcal X$ 到 $\mathbf w_i \in \mathcal W$ 的映射：$\mathbf w_i = f(\mathbf x_i) + n(\mathbf x_i)$。</li>
</ol>
<h4 id="Distributional-Representation-1"><a href="#Distributional-Representation-1" class="headerlink" title="Distributional Representation"></a>Distributional Representation</h4><p>接下来通过 data uncertainty regression 估计 $f(\mathbf x_i)$ 和 $n(\mathbf x_i)$：将 $\mathbf w_c$ 视作 target，我们应为每个 $\mathbf x_i$ 最大化如下 likelihood：</p>
<img src="https://i.loli.net/2020/04/21/cxvXulP2NjOUdrI.png" alt="image-20200421102257681" style="zoom:50%;" />

<p>实际上，我们采用 log likelihood：</p>
<img src="https://i.loli.net/2020/04/21/AuCJilcwqbX1yot.png" alt="image-20200421102318903" style="zoom:50%;" />

<p>likelihood 最大化改写为损失函数最小化：</p>
<img src="https://i.loli.net/2020/04/21/sRmZr2bPhVSEovF.png" alt="image-20200421102453700" style="zoom:50%;" />


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/10/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><span class="page-number current">11</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/12/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">zhbli</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">119</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">zhbli</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
