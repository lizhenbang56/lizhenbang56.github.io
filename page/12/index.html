<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="zhbli">
<meta property="og:url" content="http://yoursite.com/page/12/index.html">
<meta property="og:site_name" content="zhbli">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="zhbli">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/page/12/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>zhbli</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">zhbli</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-fw fa-calendar"></i>Schedule</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>Sitemap</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="fa fa-fw fa-heartbeat"></i>Commonweal 404</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/21/arXiv1611-A-Discriminatively-Learned-CNN-Embedding-for-Person-Re-identification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhbli">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="zhbli">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/21/arXiv1611-A-Discriminatively-Learned-CNN-Embedding-for-Person-Re-identification/" class="post-title-link" itemprop="url">[arXiv1611] A Discriminatively Learned CNN Embedding for Person123 Re-identification</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-04-21 17:59:11" itemprop="dateCreated datePublished" datetime="2020-04-21T17:59:11+08:00">2020-04-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-22 10:51:18" itemprop="dateModified" datetime="2020-04-22T10:51:18+08:00">2020-04-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>本文回顾了两种 reid 网络结构：verification model 和 identification model。这两种模型各有优缺点。本文结合了这两种模型：提出孪生网络同时计算 identification loss 和 verification loss。给定一个图像对，网络预测两幅图像的身份，同时预测它们是否同一身份。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Verification model 的问题：仅使用 weak reid labels，没有充分利用标签信息，因此没有考虑图像对与数据集中其他图像之间的关系。</p>
<p>为了充分利用 reid 标签，identification models 将 reid 视为多类识别问题。缺点是训练任务与测试不完全一致。</p>
<img src="https://i.loli.net/2020/04/22/Z3jKr42lFUHna9q.png" alt="image-20200422104418316" style="zoom:50%;" />

<img src="https://i.loli.net/2020/04/22/8heMbH7zpnFB94N.png" alt="image-20200422104845255" style="zoom:50%;" />
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/21/CVPR2019-Joint-Discriminative-and-Generative-Learning-for-Person-Re-identification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhbli">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="zhbli">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/21/CVPR2019-Joint-Discriminative-and-Generative-Learning-for-Person-Re-identification/" class="post-title-link" itemprop="url">[CVPR2019] Joint Discriminative and Generative Learning for Person Re-identification</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-04-21 15:57:21 / Modified: 18:58:52" itemprop="dateCreated datePublished" datetime="2020-04-21T15:57:21+08:00">2020-04-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>首次提出，将 reid 学习和数据生成整合到一个端到端框架中，名为 GD-Net。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/NVlabs/DG-Net">https://github.com/NVlabs/DG-Net</a></p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="Generative-Module"><a href="#Generative-Module" class="headerlink" title="Generative Module"></a>Generative Module</h3><p>generative module：</p>
<ul>
<li>输入：训练集中的两幅真实图像。其中一幅真实图像用于提供 appearance code，另一幅真实图像用于提供 structure code。structure code 的空间分辨率更高，以保留几何和位置属性。</li>
<li>输出：新的行人图像。</li>
<li>结构<ul>
<li>appearance encoder $E_a:x_i \rightarrow a_i$</li>
<li>structure encoder $E_s:x_j \rightarrow s_j$</li>
<li>decoder $G: (a_i,s_j)\rightarrow x_j^i$</li>
<li>discriminator $D$：用于区分图像是生成的还是真实的。</li>
</ul>
</li>
<li>objectives<ul>
<li>self-identity generation，用于规范 generator。</li>
<li>cross-identity generation，是生成的图像可控并匹配帧数数据分布。</li>
</ul>
</li>
</ul>
<h4 id="Self-identity-generation"><a href="#Self-identity-generation" class="headerlink" title="Self-identity generation"></a>Self-identity generation</h4><p>给定个一幅图像，generative module 首先学习如何重构这幅输入图像。这在整个生成过程中起到了重要的正则化作用。同身份、同图像重构损失如下：</p>
<img src="https://i.loli.net/2020/04/21/2GyKsJvF8EXZm3w.png" alt="image-20200421162403905" style="zoom:50%;" />

<p>我们还假设对于同一类别的两个行人图像 $x_i,x_t$，使用 $x_i$ 的结构信息以及 $x_t$ 的表观信息就能重构 $x_i$ 的表观信息。同身份、跨图像重构损失如下：</p>
<img src="https://i.loli.net/2020/04/21/D9BnNU4V7iqwvXz.png" alt="image-20200421162704711" style="zoom:50%;" />

<p>为了保证不同图像的 appearance code 不同，使用 identification loss 区分不同身份：</p>
<img src="https://i.loli.net/2020/04/21/g5yzAWNhDrcbJQs.png" alt="image-20200421163100753" style="zoom:50%;" />

<p>其中 $p(y_i|x_i)$ 是基于 appearance code 预测的 $x_i$ 输入 ground truth class $y_i$ 的概率。</p>
<h4 id="Cross-identity-generation"><a href="#Cross-identity-generation" class="headerlink" title="Cross-identity generation"></a>Cross-identity generation</h4><p>使用不同身份生成图像。因此没有 pixel-level 的监督信号。相反，我们引入 latent code reconstruction，基于 appearance code 和 structure code 来控制图像生成。我们希望生成的图像 $x^i_j=G(a_i,s_j)$ 具有 $x_i$ 的表观和 $x_j$ 的结构。因此我们在得到生成图像后重构 appearance code 和 structure code：</p>
<img src="https://i.loli.net/2020/04/21/QiHmtA2DjERqfPT.png" alt="image-20200421163855378" style="zoom:50%;" />

<p>此外，为了保持身份一致性，使用生成图像的 appearance code 计算 identification loss：</p>
<img src="https://i.loli.net/2020/04/21/Kl4vXuZLVkbpm7c.png" alt="image-20200421164219836" style="zoom:50%;" />

<p>另外，我们还用对抗损失使得生成图像与真实图像的分布相同：</p>
<img src="https://i.loli.net/2020/04/21/Wx1mhf4gBctA5Ly.png" alt="image-20200421164342052" style="zoom:50%;" />

<h3 id="Discriminative-Module"><a href="#Discriminative-Module" class="headerlink" title="Discriminative Module"></a>Discriminative Module</h3><h4 id="Primary-feature-learning"><a href="#Primary-feature-learning" class="headerlink" title="Primary feature learning"></a>Primary feature learning</h4><p>我们最小化如下两者的 KL 散度：</p>
<ol>
<li>由 discriminative module 预测的概率分布 $p(x^i_j)$</li>
<li>由 teacher 预测的概率分布 $q(x^i_j)$</li>
</ol>
<img src="https://i.loli.net/2020/04/21/LKe1QjYX3HcyTaV.png" alt="image-20200421165125391" style="zoom:50%;" />

<h3 id="Fine-grained-feature-mining"><a href="#Fine-grained-feature-mining" class="headerlink" title="Fine-grained feature mining"></a>Fine-grained feature mining</h3><img src="https://i.loli.net/2020/04/21/PmBgW2JFAvQux9K.png" alt="image-20200421165610583" style="zoom:50%;" />
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/21/CVPR2019-ArcFace-Additive-Angular-Margin-Loss-for-Deep-Face-Recognition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhbli">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="zhbli">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/21/CVPR2019-ArcFace-Additive-Angular-Margin-Loss-for-Deep-Face-Recognition/" class="post-title-link" itemprop="url">[CVPR2019] ArcFace: Additive Angular Margin Loss for Deep Face Recognition</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-04-21 13:45:14 / Modified: 15:35:46" itemprop="dateCreated datePublished" datetime="2020-04-21T13:45:14+08:00">2020-04-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Centre loss 为实现类内紧凑型，在欧氏空间中惩罚特征到类中心的距离。</p>
<p>SphereFase 假定最后一个全连接层中的线性变换矩阵可以被作用角度空间中类中心的表示。因此以 multiplicative 方式惩罚特征和对应权重的角度。</p>
<p>最近，为了最大化人脸类别的可分离性，将 margins 整合到以后的损失中。</p>
<p>本文提出 Additive Angular Margin Loss（ArcFace）来获得高度判别行的特征。ArcFace 具有清晰的几何解释，因为它与超球上的测地距离具有确切的对应。</p>
<p>本算法取得了最好的性能，同时增加的计算量几乎可以忽略不计。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/deepinsight/insightface">https://github.com/deepinsight/insightface</a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>主要有两种方法训练人脸识别网络：</p>
<ol>
<li><p>使用 softmax 训练多类分类器。</p>
<ul>
<li>softmax loss 的缺点：<ol>
<li>线性变化矩阵的尺寸随类别数而增加。</li>
<li>难以适用于开集人脸识别任务。</li>
</ol>
</li>
<li>改进：<ul>
<li>centre loss（ECCV2016）。缺点：训练时更新实际的中心很困难，因为人脸类别很多。</li>
<li>Sphereface（CVPR2017）。缺点：损失函数需要一系列近似，导致训练不稳定。为了稳定训练，与标准 softmax 混合训练。然而这又容易使 softmax 损失主导训练过程。</li>
<li>CosFace（CVPR2018）。</li>
</ul>
</li>
</ul>
</li>
<li><p>直接学习 embedding（利用，使用 triplet loss）。triplet loss 的缺点：</p>
<ol>
<li>大规模数据集中三元组数量会组合爆炸，导致更多的迭代次数。</li>
<li>semi-hard sample mining 很困难。</li>
</ol>
</li>
</ol>
<p>本文方法的优点总结如下：</p>
<ul>
<li>Engaging：通过归一化超球面中角度和弧度之间的精确对应关系，直接优化了 geodesic distance margin。</li>
<li>Effective：性能最优。</li>
<li>Easy：仅需几行代码。不需要与其他损失函数整合以稳定训练，在任何训练集上都易于训练。</li>
<li>Efficient：计算量很小，可以轻松对数百万个身份进行训练。</li>
</ul>
<h2 id="Proposed-Approach"><a href="#Proposed-Approach" class="headerlink" title="Proposed Approach"></a>Proposed Approach</h2><h3 id="ArcFace"><a href="#ArcFace" class="headerlink" title="ArcFace"></a>ArcFace</h3><p>最常用的 softmax 分类损失定义如下：</p>
<img src="https://i.loli.net/2020/04/21/M4UOceLat8jmFzA.png" alt="image-20200421151700044" style="zoom:50%;" />

<p>其中，特征维度是 512。</p>
<p>softmax 的缺点：未明确优化 embedding，以使得类内样本更相似，类间样本更多样。从而导致当类内表观变化大或者测试集很大时性能不佳。</p>
<p>接下来对损失改进，使得学得的特征分布在半径为 $s$ 的超球上：</p>
<img src="https://i.loli.net/2020/04/21/XfRs9zDEavAlSQo.png" alt="image-20200421151725782" style="zoom:50%;" />

<p>进一步，我们通过添加 margin penalty 改善类内的紧凑性和类间的可分性。</p>
<img src="https://i.loli.net/2020/04/21/qDaz3XLM9PTVIRY.png" alt="image-20200421151857337" style="zoom:50%;" />

<h3 id="Comparison-with-SphereFace-and-CosFace"><a href="#Comparison-with-SphereFace-and-CosFace" class="headerlink" title="Comparison with SphereFace and CosFace"></a>Comparison with SphereFace and CosFace</h3><p>margin penalty 不同：</p>
<ul>
<li>multiplicative angular margin：SphereFace</li>
<li>additive angular margin：ArcFace</li>
<li>additive cosine margin：CosFase</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/21/CVPR2020-Data-Uncertainty-Learning-in-Face-Recognition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhbli">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="zhbli">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/21/CVPR2020-Data-Uncertainty-Learning-in-Face-Recognition/" class="post-title-link" itemprop="url">[CVPR2020] Data Uncertainty Learning in Face Recognition</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-04-21 10:16:21 / Modified: 13:22:54" itemprop="dateCreated datePublished" datetime="2020-04-21T10:16:21+08:00">2020-04-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>现有人脸识别算法的问题：对于 noisy images，建模数据不确定性是重要的，但是在人脸识别中很少被研究。先前工作 [35] 把每个人脸嵌入建模为高斯分布从而考虑不确定性。然而它使用来自现有模型的固定特征（高斯的均值）。这仅估计了方差，并依赖与专门设计的、计算量高的度量方法，因此不易使用。目前仍不清楚不确定性如何影响特征学习。</p>
<blockquote>
<p>Yichun Shi, Anil K Jain, and Nathan D Kalka. <strong>Probabilistic face embeddings</strong>. In Proceedings of the IEEE International Conference on Computer Vision, 2019.</p>
</blockquote>
<p>本文将数据不确定性学习应用于人脸识别，首次使得特征（均值）和不确定性（方差）可以同时学习。提出两种学习方法，易于使用且性能良好。</p>
<p>我们还分析了整合不确定性估计如何帮助减少噪声样本的负面影响，以及如何影响特征学习的。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>数据不确定性捕获数据的固有噪声。建模这种不确定性是重要的，因为噪声广泛存在于图像中。</p>
<p>大多数人脸识别方法将每张人脸图像表示为隐空间中一个确定性的嵌入点。通常，高质量的图像中，相同 ID 的人脸图像的特征是会聚集在一起的。然而很难为有噪声的人脸图像估计准确的嵌入点，通常位于 cluster 之外，并在嵌入空间中有很大的不确定性。</p>
<p>Probabilistic face embeddings（PFE）[35] 是首先考虑人脸识别中数据不确定性的工作。 对于每个样本，在隐空间中估计高斯分布，而不是一个固定点。具体而言，给定预训练的 FR 模型，每个样本的高斯均值固定为 FR 模型产生的嵌入。在 FR 模型添加并训练额外的分支以估计方差。训练通过新的相似性度量来进行：mutual likelihood score（MLS），用于度量两个高斯分布的 likelihood。PFE 为高质量的样本估计小的方差，为噪声样本估计大的方差。因此 PFE 可以减低噪声样本的错误匹配。</p>
<p>然而，PFE 仅学习不确定性，未学习嵌入特征（mean），因此不知道不确定性如何影响特征学习。同时传统的相似性度量如 cosine 距离无法使用。需要更复杂的 MLS 度量，增加了运行时间和内存。</p>
<p>我们首次将数据不确定性学习（DUL）引入人脸识别，使得特征（均值）和不确定性（方差）可以同时学习。这改善了特征，使得同类特征更紧凑。学习的特征可以直接使用传统相似性度量，不再需要 MLS 度量。</p>
<p>具体而言，我们提出两个学习方法：</p>
<ol>
<li>第一个方法是基于分类的，从头学习一个一个模型。</li>
<li>第二个方法是基于回归的，用于改善现有模型。</li>
</ol>
<p>我们从图像噪声的角度，讨论了学习的不确定性是如何影响这两种方法的模型训练的：学习的不确定性通过自适应降低噪声训练样本的负面影响，来改善特征嵌入的学习。</p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>很多方法引入的 deep uncertainty learning，用于改进模型的鲁棒性和可解释性：</p>
<ul>
<li>语义分割<ul>
<li>Shuya Isobe and Shuichi Arai. <strong>Deep convolutional encoderdecoder network with model uncertainty for semantic segmentation</strong>. In 2017 IEEE International Conference on INnovations in Intelligent SysTems and Applications (INISTA), pages 365–370. IEEE, 2017.</li>
<li>Alex Kendall, Vijay Badrinarayanan, and Roberto Cipolla. <strong>Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding</strong>. BMVC, 2015.</li>
</ul>
</li>
<li>目标检测<ul>
<li>Jiwoong Choi, Dayoung Chun, Hyun Kim, and Hyuk-Jae Lee. <strong>Gaussian yolov3: An accurate and fast object detector using localization uncertainty for autonomous driving</strong>. In The IEEE International Conference on Computer Vision (ICCV), October 2019.</li>
<li>Florian Kraus and Klaus Dietmayer. <strong>Uncertainty estimation in one-stage object detection</strong>. arXiv preprint arXiv:1905.10296, 2019.</li>
</ul>
</li>
<li>Re-ID<ul>
<li>Tianyuan Yu, Da Li, Yongxin Yang, Timothy M Hospedales, and Tao Xiang. <strong>Robust person re-identification by modelling feature uncertainty</strong>. In Proceedings of the IEEE International Conference on Computer Vision, pages 552–561, 2019.</li>
</ul>
</li>
</ul>
<p>人脸识别中，有对模型不确定性和数据不确定性的研究：</p>
<ul>
<li>模型不确定性<ul>
<li>Sixue Gong, Vishnu Naresh Boddeti, and Anil K Jain. <strong>On the capacity of face representation</strong>. arXiv preprint arXiv:1709.10433, 2017.</li>
<li>Umara Zafar, Mubeen Ghafoor, Tehseen Zia, Ghufran Ahmed, Ahsan Latif, Kaleem Razzaq Malik, and Abdullahi Mohamud Sharif. <strong>Face recognition with bayesian convolutional networks for robust surveillance systems</strong>. EURASIP Journal on Image and Video Processing, 2019(1):10, 2019.</li>
<li>Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. <strong>Striking the right balance with uncertainty</strong>. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 103–112, 2019.</li>
</ul>
</li>
<li>数据不确定性<ul>
<li>Yichun Shi, Anil K Jain, and Nathan D Kalka. <strong>Probabilistic face embeddings</strong>. In Proceedings of the IEEE International Conference on Computer Vision, 2019.</li>
</ul>
</li>
</ul>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><h3 id="Classification-based-DUL-for-FR"><a href="#Classification-based-DUL-for-FR" class="headerlink" title="Classification-based DUL for FR"></a>Classification-based DUL for FR</h3><h4 id="Distributional-Representation"><a href="#Distributional-Representation" class="headerlink" title="Distributional Representation"></a>Distributional Representation</h4><p>将每个样本 $\mathbf{x}_i$ 在隐空间中的特征表示 $\mathbf{z}_i$ 定义为高斯分布：</p>
<img src="https://i.loli.net/2020/04/21/HcBsDzOleWUp5N3.png" alt="image-20200421101935936" style="zoom:50%;" />

<p>其中，高斯分布的均值和方差都与输入相关，并通过 CNN 预测：</p>
<img src="https://i.loli.net/2020/04/21/MjtHph4kBedfzQC.png" alt="image-20200421111911376" style="zoom:50%;" />

<p>此处预测的高斯是 diagonal multivariate normal。$\pmb{\mu}_i$ 可被视作人脸的理想特征，$\pmb{\sigma}_i$ 可视作不确定性。现在，每个样本的表示不再是固定的嵌入点，而是从高斯分布中随机采样的嵌入。</p>
<p>然而采样操作是无法反向传播的。我们使用 re-parameterization trick [24] 使得模型仍然能计算梯度。具体而言，首先从正太分布中采样与模型参数无关的随机噪声 $\epsilon$，然后生成 $\mathbf{s}_i$ 作为等效的特征表示（公式2）：</p>
<img src="https://i.loli.net/2020/04/21/9VK7SxLcGrEw1Z5.png" alt="image-20200421101958743" style="zoom:50%;" />

<p>其中，$\mathbf{s}_i$ 是图像的最终表示。</p>
<h4 id="Classification-Loss"><a href="#Classification-Loss" class="headerlink" title="Classification Loss"></a>Classification Loss</h4><p>使用分类器最小化 softmax 损失（公式3）：</p>
<img src="https://i.loli.net/2020/04/21/gPpFnJNcm14ojk6.png" alt="image-20200421102125834" style="zoom:50%;" />

<p>实际上，使用 softmax 损失的不同变种来训练分类模型：</p>
<ul>
<li>additive margin：Feng Wang, Jian Cheng, Weiyang Liu, and Haijun Liu. <strong>Additive margin softmax for face verification</strong>. IEEE Signal Processing Letters, 25(7):926–930, 2018.</li>
<li>feature $\ell2$ normalization： Rajeev Ranjan, Carlos D Castillo, and Rama Chellappa. <strong>L2-constrained softmax loss for discriminative face verification</strong>. arXiv preprint arXiv:1703.09507, 2017.</li>
<li>arcface：Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. <strong>Arcface: Additive angular margin loss for deep face recognition</strong>. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4690– 4699, 2019.</li>
</ul>
<h4 id="KL-Divergence-Regularization"><a href="#KL-Divergence-Regularization" class="headerlink" title="KL-Divergence Regularization"></a>KL-Divergence Regularization</h4><p>公式 2 表明，在训练时所有特征嵌入都会被 $\pmb{\sigma}_i$ 破坏，这会让网络对所有样本都预测小的 $\pmb \mu_i$ 来抑制 $\mathbf{s}_i$ 的不稳定成分，这样公式 3 仍然能收敛，不过退退化成原始的确定性表示。</p>
<p>受 variational information bottleneck [1] 的启发，在优化时引入正则项：显式约束 $\mathcal{N}(\pmb \mu_i, \pmb \sigma_i)$ 为正态分布，者通过 KL 散度来度量：</p>
<img src="https://i.loli.net/2020/04/21/AWohfxjNg9DPHSX.png" alt="image-20200421102221253" style="zoom:50%;" />

<blockquote>
<p>Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. <strong>Deep variational information bottleneck</strong>. In Proceedings of the International Conference on Learning Representations, 2017.</p>
</blockquote>
<h3 id="Regression-based-DUL-for-FR"><a href="#Regression-based-DUL-for-FR" class="headerlink" title="Regression-based DUL for FR"></a>Regression-based DUL for FR</h3><h4 id="Difficulty-of-Introducing-Data-Uncertainty-Regression-to-FR"><a href="#Difficulty-of-Introducing-Data-Uncertainty-Regression-to-FR" class="headerlink" title="Difficulty of Introducing Data Uncertainty Regression to FR"></a>Difficulty of Introducing Data Uncertainty Regression to FR</h4><p>由于在人脸的映射空间 $\mathcal{X \rightarrow Y}$ 中，$\mathcal{X}$ 是连续的，但 $\mathcal{Y}$ 是离散的，因此不能直接应用数据不确定性回归。</p>
<h4 id="Constructing-New-Mapping-Space-for-FR"><a href="#Constructing-New-Mapping-Space-for-FR" class="headerlink" title="Constructing New Mapping Space for FR"></a>Constructing New Mapping Space for FR</h4><p>我们为人脸数据构建了连续的 target space，这与原始的离散 target space 几乎等价。步骤如下：</p>
<ol>
<li>预训练基于分类的确定性 FR 模型。</li>
<li>利用分类模型的分类层 $\mathcal W \in \mathbb R^{D\times C}$ 作为 expected target vector。其中 $D$ 是嵌入的维度，$C$ 是训练集的类别数。</li>
<li>由于每个 $\mathbf w_i \in \mathcal W$ 可以看作具有相同类别的嵌入的 typical center，${\mathcal{X,W}}$ 可以看作新的 equivalent mapping space。${\mathcal{X,W}}$ 同样具有固有噪声。</li>
<li>我们可以建立从 $\mathbf x_i \in \mathcal X$ 到 $\mathbf w_i \in \mathcal W$ 的映射：$\mathbf w_i = f(\mathbf x_i) + n(\mathbf x_i)$。</li>
</ol>
<h4 id="Distributional-Representation-1"><a href="#Distributional-Representation-1" class="headerlink" title="Distributional Representation"></a>Distributional Representation</h4><p>接下来通过 data uncertainty regression 估计 $f(\mathbf x_i)$ 和 $n(\mathbf x_i)$：将 $\mathbf w_c$ 视作 target，我们应为每个 $\mathbf x_i$ 最大化如下 likelihood：</p>
<img src="https://i.loli.net/2020/04/21/cxvXulP2NjOUdrI.png" alt="image-20200421102257681" style="zoom:50%;" />

<p>实际上，我们采用 log likelihood：</p>
<img src="https://i.loli.net/2020/04/21/AuCJilcwqbX1yot.png" alt="image-20200421102318903" style="zoom:50%;" />

<p>likelihood 最大化改写为损失函数最小化：</p>
<img src="https://i.loli.net/2020/04/21/sRmZr2bPhVSEovF.png" alt="image-20200421102453700" style="zoom:50%;" />


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/20/CVPR2020-X3D-Expanding-Architectures-for-Efficient-Video-Recognition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhbli">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="zhbli">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/20/CVPR2020-X3D-Expanding-Architectures-for-Efficient-Video-Recognition/" class="post-title-link" itemprop="url">[CVPR2020] X3D: Expanding Architectures for Efficient Video Recognition</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-04-20 16:23:58 / Modified: 20:22:08" itemprop="dateCreated datePublished" datetime="2020-04-20T16:23:58+08:00">2020-04-20</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>本文提出 X3D，在多个 network axes 上（space/time/width/depth），逐渐扩展 2D 图像分类网络。受机器学习中特征选择方法的启发，提出 stepwise network expansion 方法，在每个 step 扩展一个 single axis。为了将 X3D 扩展到指定的 target complexity，我们执行 progressive forward expansion 和 backward contraction。</p>
<p>我们最惊奇的发现是，具有高时空分辨率的网络性能很好，同时网络宽度和参数很小。</p>
<p>性能：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/SlowFast">https://github.com/facebookresearch/SlowFast</a></li>
<li>在达到最优性能的同时，加/乘法运算少了 4.8倍，参数少了 5.5 倍。</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>网络的深度指网络层数，宽度指通道数。</p>
<p>沿如下轴扩展网络：</p>
<ul>
<li>temporal duration $\gamma_t$</li>
<li>frame rate $\gamma_\tau$</li>
<li>spatial resolution $\gamma_s$</li>
<li>width $\gamma_w$</li>
<li>bottleneck width $\gamma_b$</li>
<li>depth $\gamma_d$</li>
</ul>
<p>2D 基础结构是 MobileNet。每次扩展一个轴，训练并验证，选择实现最佳计算量与性能折中的轴。重复执行这一操作以达到想要的 computational budget。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/20/CVPR2020-Learning-Representations-by-Predicting-Bags-of-Visual-Words/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhbli">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="zhbli">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/20/CVPR2020-Learning-Representations-by-Predicting-Bags-of-Visual-Words/" class="post-title-link" itemprop="url">[CVPR2020] Learning Representations by Predicting Bags of Visual Words</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-04-20 14:38:42 / Modified: 16:09:26" itemprop="dateCreated datePublished" datetime="2020-04-20T14:38:42+08:00">2020-04-20</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>提出基于 spatially dense image descriptions 的自监督方法，编码了离散的视觉概念（visual words）。</p>
<p>为了建立这种离散表示，我们通过基于词表的 k-means，对预训练的自监督网络的特征图进行量化。然后我们自监督训练另一个网络，预测图像的 visual words 的直方图（Bags-of-Words），该网络的输入是经过扰动的图像。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><p><img src="https://i.loli.net/2020/04/20/se2IuhaMJ438yHL.png" alt="image-20200420144105047"></p>
<p>我们的目的是，以无监督形式训练网络 $\Phi(\cdot)$，使得为图像产生“好”的特征表示。“好”指的是，有利于图像分类，目标检测等视觉任务。</p>
<p>我们假设有一个可用的初始自监督预训练网络 $\hat \Phi(\cdot)$，即 RotNet。</p>
<p>算法流程：</p>
<ol>
<li>我们利用 $\hat \Phi(\cdot)$ 创建基于 visual words 的 spatially dense descriptions。</li>
<li>将这些 descriptions 聚合为 BoW representations。</li>
<li>训练模型 $\Phi(\cdot)$ 以重构 BoW，输入为扰动的图像。</li>
<li>注意，训练 $\Phi(\cdot)$ 时，$\hat \Phi(\cdot)$ 保持冻结。</li>
<li>训练了 $\Phi(\cdot)$ 之后，设置 $\hat \Phi(\cdot) \leftarrow\Phi(\cdot)$ 并重复训练过程。</li>
</ol>
<h3 id="Building-spatially-dense-discrete-descriptions"><a href="#Building-spatially-dense-discrete-descriptions" class="headerlink" title="Building spatially dense discrete descriptions"></a>Building spatially dense discrete descriptions</h3><p>给定一幅训练图像 $\mathbf{x}$，算法第一步是使用 $\hat \Phi(\cdot)$创建 spatially dense visual words-based description $q(\cdot)$。</p>
<p>设 $\hat \Phi(\mathbf{x})$ 是特征图，通道为 $\hat c$，空间分辨率为 $\hat h \times \hat w$。$\hat \Phi^u(\mathbf{x})$ 是位于位置 $u$ 的特征向量。$U=\hat h \times \hat w$。</p>
<p>为了得到 description $q(\mathbf{x}) = [q^1(\mathbf{x}), …, q^U(\mathbf{x})])$，我们使用预定义词表 $V=[\mathbf{v}_1, …, \mathbf{v}_K]$ 对 $\hat \Phi(\mathbf{x})$ 执行密集量化。该词表包含 $\hat c$ 维的 visual word embeddings，$K$ 是词表尺寸。</p>
<p>具体而言，为每个特征向量 $\hat \Phi^u(\mathbf{x})$ 根据最近的欧氏距离分类一个 visual word embedding $q^u(\mathbf{x})$：</p>
<img src="https://i.loli.net/2020/04/20/nSV7EXKhcomHTgA.png" alt="image-20200420144132506" style="zoom:50%;" />

<p>注意，$q^u(\mathbf{x})$ 是一个标量。</p>
<p>词表 $V$ 的学习：在数据集 $X$ 的特征图的集合上，应用 k-means 算法，clusters 为 $K$：</p>
<img src="https://i.loli.net/2020/04/20/38BUWAQ6DcSsgGf.png" alt="image-20200420153412621" style="zoom:50%;" />

<p>其中 visual word embedding $\mathbf{v}_k$ 是 $k$-th cluster 的中心。</p>
<h3 id="Generating-Bag-of-Words-representations"><a href="#Generating-Bag-of-Words-representations" class="headerlink" title="Generating Bag-of-Words representations"></a>Generating Bag-of-Words representations</h3><p>得到 discrete description $q(\mathbf{x})$ 后，下一步是创建它的 BoW representation $y(\mathbf{x})$。这是一个 $K$ 维向量，元素 $y^k(\mathbf{x})$ 要么表示 $k-th$ visual word 出现的次数：</p>
<img src="https://i.loli.net/2020/04/20/MQvL2Pzxc5ugieI.png" alt="image-20200420144219450" style="zoom:50%;" />

<p>要么表示 $k$-th visual word 是否在图像中出现：</p>
<img src="https://i.loli.net/2020/04/20/9e6xdLshky2IJ4Q.png" alt="image-20200420154442474" style="zoom:50%;" />

<p>为了将 $y^k(\mathbf{x})$ 转为 visual words 上的概率分布，使用 $L_1$ 正则化：</p>
<img src="https://i.loli.net/2020/04/20/MOgzVDr3djlSIQa.png" alt="image-20200420154610839" style="zoom:50%;" />

<p>因此 $y(\mathbf{x})$ 可以解释为 $K$ 个 visual words 的 soft categorical label。</p>
<h3 id="Learning-to-“reconstruct”-BoW"><a href="#Learning-to-“reconstruct”-BoW" class="headerlink" title="Learning to “reconstruct” BoW"></a>Learning to “reconstruct” BoW</h3><p>基于上述的 BoW representation，我们提出如下自监督任务：给定输入图片 $\mathbf{x}$，执行扰动操作 $g(\cdot)$，得到扰动的图像 $\tilde{\mathbf{x}} = g(\mathbf{x})$，然后训练模型来预测/重构原始图像 $\mathbf{x}$ 的 BoW representation $y(\mathbf{x})$。</p>
<p>我们希望通过特征向量 $\Phi(\tilde{\mathbf{x}}) \in \mathbb{R}^c$ 预测  BoW representation $y(\mathbf{x})$。</p>
<p>为此，我们定义 prediction layer $\Omega(\cdot)$，将 $\Phi(\tilde{\mathbf{x}})$ 作为输入，输出 BoW representatoin 的 $K$ 个 visual words 的 $K$ 维 softmax distribution。具体而言，prediction layer 是通过线性层加 softmax 层实现的：</p>
<img src="https://i.loli.net/2020/04/20/R7OT2m9hIHKXalr.png" alt="image-20200420144337253" style="zoom:50%;" />

<h4 id="Self-supervised-training-objective"><a href="#Self-supervised-training-objective" class="headerlink" title="Self-supervised training objective"></a>Self-supervised training objective</h4><p>损失函数为交叉熵损失：</p>
<img src="https://i.loli.net/2020/04/20/LJkFxOuiZ9hYUKt.png" alt="image-20200420144407297" style="zoom:50%;" />

<h4 id="分析"><a href="#分析" class="headerlink" title="## 分析"></a>## 分析</h4><p>Q：如何保证经过训练的 $\Phi(\cdot)$ 的特征表示，比 $\hat \Phi(\cdot)$ 更好呢？</p>
<p>A：关键在于图像扰动。也就是说，如果没有图像扰动，无法让 $\Phi(\cdot)$ 的特征比 $\hat \Phi(\cdot)$ 更好，而倾向于使 $\Phi(\cdot)$ 与 $\hat \Phi(\cdot)$ 恒等。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/20/arXiv2004-A-Simple-Baseline-for-Multi-Object-Tracking/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhbli">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="zhbli">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/20/arXiv2004-A-Simple-Baseline-for-Multi-Object-Tracking/" class="post-title-link" itemprop="url">[arXiv2004] A Simple Baseline for Multi-Object Tracking</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-04-20 13:30:11 / Modified: 14:31:52" itemprop="dateCreated datePublished" datetime="2020-04-20T13:30:11+08:00">2020-04-20</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>现有 MOT 算法的问题：很少有算法在单个网络中同时进行目标检测和 reid。现有的尝试导致性能降低。</p>
<p>原因：现有的尝试未能正确学习 reid 分支。</p>
<p>本文的解决方案：分析了未能正确学习 reid 分支的原因，并提出了改进。</p>
<p>性能：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/ifzhang/FairMOT">https://github.com/ifzhang/FairMOT</a></li>
<li>速度：30 FPS。</li>
<li>MOT17_MOTA：67.5（private_detector）。</li>
<li>MOT20_MOTA：58.7（private_detector）。</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>分析了在单个网络中同时进行目标检测和 reid 的三个关键因素：</p>
<ol>
<li>anchors 不适用于 reid。<ol>
<li>原因：<ol>
<li>表示不同图像块的多个 anchors 可能对应于同一个物体，这导致网络的严重歧义。</li>
<li>下采样为 8，这对 reid 来说太粗糙了。这导致目标中心与特征中心无法对齐。</li>
</ol>
</li>
<li>方案：<ul>
<li>将 MOT 问题视为在高分辨率特征图上的 pixel-wise 关键点（即目标中心）估计问题和 identity classification 问题。</li>
</ul>
</li>
</ol>
</li>
<li>多层特征聚合。<ul>
<li>这对 MOT 很重要，因为 reid 需要利用低层和高层特征来处理小目标和大目标，从而提成了对物体尺度变化的适应性。</li>
</ul>
</li>
<li>reid 特征的维度。<ul>
<li>低维特征对 MOT 更好。因为训练数据少，而我们无法使用 reid 数据集。减少了过拟合风险，提高了鲁棒性。</li>
</ul>
</li>
</ol>
<h2 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h2><p>MOT 分为两阶段跟踪和一阶段跟踪：</p>
<ul>
<li>两阶段跟踪<ul>
<li>优点：可以针对每个任务使用最佳模型而无需折中。</li>
<li>缺点：速度慢，无法实现权重共享。</li>
</ul>
</li>
<li>一阶段跟踪<ul>
<li>优点：速度块，权重共享。</li>
<li>缺点：无法很好地学习 reid 分支。</li>
</ul>
</li>
</ul>
<h2 id="The-Technical-Approach"><a href="#The-Technical-Approach" class="headerlink" title="The Technical Approach"></a>The Technical Approach</h2><p>网络分为三部分：</p>
<ol>
<li>backbone</li>
<li>目标检测分支</li>
<li>reid 分支</li>
</ol>
<h3 id="Backbone-Network"><a href="#Backbone-Network" class="headerlink" title="Backbone Network"></a>Backbone Network</h3><p>使用基于 DLA 的变种 [45] 的 resnet34，下采样率为 4。</p>
<blockquote>
<p>Zhou, X., Wang, D., Kr¨ahenb¨uhl, P.: Objects as points. arXiv preprint arXiv:1904.07850 (2019)</p>
</blockquote>
<h3 id="Object-Detection-Branch"><a href="#Object-Detection-Branch" class="headerlink" title="Object Detection Branch"></a>Object Detection Branch</h3><p>与 [45] 相同，将目标检测视为基于中心点的边框回归任务，在 backbone 上附加 3 个  regression heads，分别用于估计 heatmaps，object center offsets 和边框尺寸。每个 head 一个是 256 通道的 $3\times 3$ 卷积，后接一个 $1\times 1$ 卷积。</p>
<h4 id="Heatmap-Head"><a href="#Heatmap-Head" class="headerlink" title="Heatmap Head"></a>Heatmap Head</h4><p>heatmap 的尺寸是 $1\times H \times W$ （与特征图尺寸相同）。Ground truth 是以目标为中心的高斯。</p>
<h4 id="Center-Offset-Head"><a href="#Center-Offset-Head" class="headerlink" title="Center Offset Head"></a>Center Offset Head</h4><p>由于下采样率为 4，必然引入误差。这对检测的影响不大，但对跟踪至关重要。因为应该根据准确的目标中心提取 reid 特征。</p>
<h4 id="Box-Size-Head"><a href="#Box-Size-Head" class="headerlink" title="Box Size Head"></a>Box Size Head</h4><p>与 reid 无关。</p>
<h3 id="Identity-Embedding-Branch"><a href="#Identity-Embedding-Branch" class="headerlink" title="Identity Embedding Branch"></a>Identity Embedding Branch</h3><p>该分支的目的是生成可以区分不同目标的特征。通过在特征图上卷积为每个位置得到 128 为嵌入向量。</p>
<h4 id="Loss-Functions"><a href="#Loss-Functions" class="headerlink" title="Loss Functions"></a>Loss Functions</h4><h4 id="Heatmap-Loss"><a href="#Heatmap-Loss" class="headerlink" title="Heatmap Loss"></a>Heatmap Loss</h4><img src="https://i.loli.net/2020/04/20/xTV9UOWnyFGZtJk.png" alt="image-20200420133241614" style="zoom:50%;" />

<h4 id="Offset-and-Size-Loss"><a href="#Offset-and-Size-Loss" class="headerlink" title="Offset and Size Loss"></a>Offset and Size Loss</h4><img src="https://i.loli.net/2020/04/20/TQsqUZlCJ2NmRHP.png" alt="image-20200420133310654" style="zoom:50%;" />

<h4 id="Identity-Embedding-Loss"><a href="#Identity-Embedding-Loss" class="headerlink" title="Identity Embedding Loss"></a>Identity Embedding Loss</h4><p>将身份嵌入视为分类任务：将训练集中具有同一人物的目标视作同一类。</p>
<img src="https://i.loli.net/2020/04/20/bfgBwLqz4cxNrQa.png" alt="image-20200420133335418" style="zoom:50%;" />

<p>$\mathbf{p}(k)$ 为 class distribution vector。</p>
<p>$\mathbf{L}^i(k)$ 为 GT class label 的 one-hot 表示。</p>
<p>$K$ 是类别数。</p>
<p>N 是图像中的目标数。</p>
<h3 id="Online-Tracking"><a href="#Online-Tracking" class="headerlink" title="Online Tracking"></a>Online Tracking</h3><p>使用 reid 特征的距离，以及 IoU，来链接两帧的边框。</p>
<h3 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h3><p>使用 DLA-34 的变种 [45] 作为 backbone。使用 coco 预训练参数对模型初始化。</p>
<p>在两个 RTX 2080 GPU 上的训练时间约 30 小时。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/19/NIPS2016-Matching-Networks-for-One-Shot-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhbli">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="zhbli">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/19/NIPS2016-Matching-Networks-for-One-Shot-Learning/" class="post-title-link" itemprop="url">[NIPS2016] Matching Networks for One Shot Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-04-19 17:41:18" itemprop="dateCreated datePublished" datetime="2020-04-19T17:41:18+08:00">2020-04-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-20 12:45:40" itemprop="dateModified" datetime="2020-04-20T12:45:40+08:00">2020-04-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>本文的框架学习一个网络，该网络将一个小的<code>有标签支持集</code>和一个<code>无标签样本</code>映射到它的标签，无需微调以适应新的 <code>class types</code>。</p>
<p>然后，本文定义了视觉和语言任务上的 one-shot learning 问题。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>One shot learning 指：使用一个有标签样本学习一个类别。</p>
<p>我们希望同时整合 parametric models 和 non-parametric models 的最佳特性，即可以快速获取 new examples，同时对 common examples 泛化良好。</p>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>我们的非参数化方法包括两个组件：</p>
<ol>
<li>给定小的支持集 $S$，我们的模型为每个 $S$ 定义一个函数（分类器） $c_S$，即一个映射 $S \rightarrow c_S(\cdot)$。</li>
<li>设计了学习策略，专用于从支持集 $S$ 中进行 one-shot learning。 </li>
</ol>
<h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p>我们希望建立从小的支持集（具有 $k$ 个样本的 input-label 对）$S={(x_i,y_i)}_{i=1}^k$ 到分类器 $c_S(\hat x)$ 的映射。该分类器给定一个 test example $\hat x$，定义输出 $\hat y$ 的概率分布。其中，$\hat x$ 是一幅图像，$\hat y$ 是图像类别的分布。我们将映射 $S \rightarrow c_S(\hat x)$ 定义为 $P(\hat y|\hat x, S)$，其中 $P$ 由神经网络进行参数化。当给出新的支持集 $S’$ 进行 one-shot learning 时，只需使用由神经网络定义的 $P$ 来为每个测试样本 $\hat x$ 预测近似标签分布 $\hat y$：$P(\hat y|\hat x,S’)$。我们的模型以最简单的方式计算 $\hat y$ 的概率分布：</p>
<img src="https://i.loli.net/2020/04/19/6lC5VkLwouOxTcR.png" alt="image-20200419210932733" style="zoom:50%;" />

<p>其中 $x_i,y_i$ 是来自支持集 $S={(x_i,y_i)}_{i=1}^k$ 的输入和标签分布。$a$ 是 attention 机制。注意，该公式的本质是将新类别的输出描述为支持集中标签的线性组合。</p>
<h4 id="The-Attention-Kernel"><a href="#The-Attention-Kernel" class="headerlink" title="The Attention Kernel"></a>The Attention Kernel</h4><p>注意力机制 $a(\cdot, \cdot)$ 的最简单形式是在余弦距离上使用 softmax：</p>
<img src="https://i.loli.net/2020/04/19/tfde1WaclBpEjXk.png" alt="image-20200419211508786" style="zoom:50%;" />

<p>其中，$f$ 和 $g$ 分别用于编码 $\hat x$ 和 $x_i$。</p>
<h4 id="Full-Context-Embeddings"><a href="#Full-Context-Embeddings" class="headerlink" title="Full Context Embeddings"></a>Full Context Embeddings</h4><p>我们认为，$g(x_i)$ 不应该仅依赖 $x_i$ 而于支持集 $S$ 的其他元素无关。因此我们提出 $g$ 不仅要依赖 $x_i$，还要依赖 $S$：因此有 $g(x_i, S)$。这在有些元素 $x_j$ 与 $x_i$ 非常接近时会有用，此时会以更好的方式编码 $x_i$。我们用双边 LSTM 在支持集 $S$ 的上下文中嵌入 $x_i$。</p>
<p>同时我们认为，$f$ 应该同时依赖 $f$ 和 $S$。可以设计一个在整个支持集 $S$ 上具有 read-attention 的 LSTM 来实现。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/18/CVPR2020-Siam-R-CNN-Visual-Tracking-by-Re-Detection/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhbli">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="zhbli">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/18/CVPR2020-Siam-R-CNN-Visual-Tracking-by-Re-Detection/" class="post-title-link" itemprop="url">[CVPR2020] Siam R-CNN: Visual Tracking by Re-Detection</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-04-18 17:58:47" itemprop="dateCreated datePublished" datetime="2020-04-18T17:58:47+08:00">2020-04-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-05-13 20:41:56" itemprop="dateModified" datetime="2020-05-13T20:41:56+08:00">2020-05-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tracking/" itemprop="url" rel="index"><span itemprop="name">Tracking</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tracking/Architecture/" itemprop="url" rel="index"><span itemprop="name">Architecture</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>SiamRCNN 具有以下特点：</p>
<ul>
<li>充分发挥两阶段检测器的作用。</li>
<li>提出 tracklet-based 动态传播算法，利用第一帧模板和先前帧预测信息，建模目标与干扰物的完整历史。这有利于更好的跟踪决策，并有利于在长期遮挡后重新跟踪物体。</li>
<li>提出难例挖掘，用于训练提出的 re-detector，以区分近似物体。</li>
</ul>
<p>性能：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/VisualComputingInstitute/SiamR-CNN">https://github.com/VisualComputingInstitute/SiamR-CNN</a></li>
<li>速度：4.7 FPS。</li>
<li>GOT-10k：0.649。</li>
</ul>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><img src="https://i.loli.net/2020/04/18/yvmBka258xpGCif.png" alt="image-20200418201225192" style="zoom:50%;" />

<h3 id="Siam-R-CNN"><a href="#Siam-R-CNN" class="headerlink" title="Siam R-CNN"></a>Siam R-CNN</h3><p>Siam R-CNN 是基于两阶段检测器的 Siamese re-detector。检测器采用 coco 预训练的 80 类 faster rcnn。我们固定特征提取器和 RPN 参数，并用我们的 re-detection head 代替 category-specific detection。</p>
<p>为 re-detection head 准备输入：</p>
<ol>
<li><p>来自 RPN 的 proposed region，经过 RoI Align 得到固定尺寸的特征。</p>
</li>
<li><p>得到第一帧 gt 目标的RoI Aligned features。</p>
</li>
<li><p>将上述两个特征串接送入 $1\times 1$ 卷积，将通道数减半，作为 re-detection head 的输入。</p>
</li>
</ol>
<p>Re-detection head 使用三级级联，不共享权重。结构与 faster rcnn 的 detection head 相同。输出类别数为 2。仅训练 re-detection head。</p>
<h3 id="Video-Hard-Example-Mining"><a href="#Video-Hard-Example-Mining" class="headerlink" title="Video Hard Example Mining"></a>Video Hard Example Mining</h3><p>在常规的 Faster R-CNN 训练期间，从目标图像中 RPN 提出的区域中采样第二阶段的负样本。但是，在许多图像中，只有很少的负样本。为了最大程度地提高 re-detection head 的判别能力，我们需要在更难得负样本上进行训练。</p>
<p>我们没有选择用于检测的 general hard examples，而是通过检索其他视频的目标，选择相对 reference object 的困难样本。</p>
<h4 id="Embedding-Network"><a href="#Embedding-Network" class="headerlink" title="Embedding Network"></a>Embedding Network</h4><p>选择难例的直接方法是寻找相同类别。但这有三个问题：</p>
<ol>
<li>类别信息不总是可用。</li>
<li>同类物体可能易于区分。</li>
<li>不同类物体可能不易于区分。</li>
</ol>
<p>我们受 reid 网络启发，提出 embedding network，从每个目标的 gt box 中提取嵌入向量，用于表示该目标的表观。我们使用 PReMVOS [60] 网络, 使用三元组损失，先在 coco 上训练以区分不同类别，后再 youtube_vos 上训练以区分不同实例。</p>
<blockquote>
<p>[60] J. Luiten, P. Voigtlaender, and B. Leibe. PReMVOS: Proposal-generation, refinement and merging for video object segmentation. In ACCV, 2018.</p>
</blockquote>
<h4 id="Index-Structure"><a href="#Index-Structure" class="headerlink" title="Index Structure"></a>Index Structure</h4><p>通过在嵌入空间查询与目标最接近的物体来寻找难例。</p>
<h4 id="Training-Procedure"><a href="#Training-Procedure" class="headerlink" title="Training Procedure"></a>Training Procedure</h4><p>每训练一帧时，在其他视频上运行</p>
<p>我们为训练集中的每个 gt box 预先计算 RoI-aligned 特征。</p>
<p>使用 index structure 选择目标的 10000 个最近邻，再从中选择 100 分作为额外的负样本。注意，这些负样本是 RoI-aligned 的特征，与目标特征串接并送入 $1\times 1$ 卷积后，作为 re-detection head 的输入。</p>
<h3 id="Tracklet-Dynamic-Programming-Algorithm"><a href="#Tracklet-Dynamic-Programming-Algorithm" class="headerlink" title="Tracklet Dynamic Programming Algorithm"></a>Tracklet Dynamic Programming Algorithm</h3><p>我们的 TDPA 利用时空信息，不仅跟踪目标，同时跟踪近似物体。</p>
<p>TDPA 维护一组 tracklets，即几乎肯定属于同一物体的短的检测序列。设计基于动态规划的评分算法，在第一帧到当前帧之间，选择最好的 <code>tracklets 序列</code>。</p>
<p>每个 detection 都是一个 tracklet 的一部分，由一个边框，重检测得分和 RoI-Aligned 特征组成。</p>
<p>一个 tracklet 由一组 detections 组成，每个 time step 对应一个 detection。</p>
<h4 id="Tracklet-Building"><a href="#Tracklet-Building" class="headerlink" title="Tracklet Building"></a>Tracklet Building</h4><p>利用第一帧的边框初始化一个 tracklet。</p>
<p>对于新一帧，以如下方式更新一组 tracklets：</p>
<ol>
<li>运行网络的 backbone 和 RPN 获得 RoIs。</li>
<li>为了补偿潜在的 RPN false negatives（当前帧的 RPN 网络没能检测出物体），通过前一帧输出的边框来扩展 RoIs。-&gt; 目的：获取当前帧中所有候选目标。</li>
<li>在这些 RoIs 上运行针对于第一帧模板的 re-detection head。</li>
<li>在当前检测上重新运行 re-detection head 的分类部分，但这次以前一帧的检测作为参考，而不是第一帧。计算每对检测的相似性。-&gt; 目的：将当前帧目标与上一帧目标进行配对。</li>
<li>仅对空间距离小于 $\gamma$ 的检测对计算相似度。</li>
<li>当新检测的得分高且没有歧义（没有其他检测具有同样高的相似性）时，将 tracklet 从上一帧扩展到当前帧。</li>
<li>只要有任何歧义，就使用该检测初始化一个新的 tracklet。</li>
<li>歧义通过 tracklet 评分步骤解决。</li>
</ol>
<h4 id="Scoring"><a href="#Scoring" class="headerlink" title="Scoring"></a>Scoring</h4><p>一个 track $A=(a_1, ,,, a_N)$ 指 $N$ 个<strong>不重叠</strong>的 <code>tracklets 序列</code>。</p>
<p>一个 track 的总分包括：</p>
<ul>
<li>unary score，用于评估每个 tracklets 的质量。</li>
<li>loc_score，惩罚 tracklets 间的空间跳跃。</li>
</ul>
<img src="https://i.loli.net/2020/04/20/RBVkCqAneQZLvUG.png" alt="image-20200420114730875" style="zoom:50%;" />

<img src="https://i.loli.net/2020/04/20/notCe13ySiscv6l.png" alt="image-20200420115107155" style="zoom:50%;" />

<p>其中 $\text{ff_score}$ 指使用第一帧作为参考的 redetection reference。</p>
<h4 id="Online-Dynamic-Programming"><a href="#Online-Dynamic-Programming" class="headerlink" title="Online Dynamic Programming"></a>Online Dynamic Programming</h4><p>通过维护一个数组 $\theta$ 来有效找到具有最高总分的 <code>tracklets 序列</code>。对每个 tracklet $a$，$\theta [a]$ 保存了从第一帧开始，以 $a$ 结束的最优 <code>tracklet 序列</code>的总分。</p>
<p>一旦一个 tracklet 不再扩展，则将终止。因此对于每个新帧，仅需重新计算扩展的或新的 tracklets 的得分。</p>
<p>对于新的 time-step，首先为第一帧的 tracklet $a_{ff}$ 设置 $\theta[a_{ff}] = 0$，因为所有的 tracks 都必须以该 tracklet 开始。</p>
<p>之后，对于每个被更新的或新创建的 tracklet $a$，$\theta[a]$ 被计算为：</p>
<img src="https://i.loli.net/2020/04/19/OSJIAB71PUma4uD.png" alt="image-20200419142508930" style="zoom: 50%;" />

<p>其中，$\tilde a$ 指在 $a$ 开始前就结束的 tracklet（即无重叠）。</p>
<p>在更新当前帧的 $\theta$ 之后，选择具有最高动态规划得分的 tracklet $\hat a = \arg \max_a\theta[a]$。如果在当前帧中选择的 tracklets 不包含检测，则说明目标不存在。对于帧都需要预测框的数据集，使用上一帧的框代替，并赋得分为 0。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/18/ICCV2019-Learning-the-Model-Update-for-Siamese-Trackers/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhbli">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="zhbli">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/18/ICCV2019-Learning-the-Model-Update-for-Siamese-Trackers/" class="post-title-link" itemprop="url">[ICCV2019] Learning the Model Update for Siamese Trackers</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-04-18 12:43:54" itemprop="dateCreated datePublished" datetime="2020-04-18T12:43:54+08:00">2020-04-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-05-13 18:21:08" itemprop="dateModified" datetime="2020-05-13T18:21:08+08:00">2020-05-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tracking/" itemprop="url" rel="index"><span itemprop="name">Tracking</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tracking/Model-Update/" itemprop="url" rel="index"><span itemprop="name">Model Update</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>现有算法的问题：孪生跟踪器中，当前帧模板与先前帧累积的模板线性组合，导致信息随时间指数衰减。尽管这种方式可以改善跟踪结果，但其简单性限制了潜在性能。</p>
<p>本文的解决方案：通过学习如何更新来代替手工设计的更新方法。提出 UpdateNet：</p>
<ul>
<li>结构：卷积网络，易于集成到现有孪生跟踪器中。</li>
<li>输入：<ul>
<li>初始模板</li>
<li>累积模板</li>
<li>当前模板</li>
</ul>
</li>
<li>输出：<ul>
<li>针对下一帧的最优模板，即新的累积模板。</li>
</ul>
</li>
</ul>
<p>性能：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/zhanglichao/updatenet">https://github.com/zhanglichao/updatenet</a></li>
<li>~50 FPS</li>
<li>baseline：DaSiamRPN</li>
<li>VOT2016_EAO：0.439 $\rightarrow$ 0.481</li>
<li>VOT2018_EAO：0.383 $\rightarrow$ 0.393</li>
<li>LaSOT_Precision：0.538 $\rightarrow$ 0.560</li>
<li>TrackingNet_Precision：59.1 $\rightarrow$ 62.5</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>传统孪生跟踪器</p>
<ul>
<li>方案：模板通过第一帧初始化，并在跟踪中保持固定。</li>
<li>缺点：难以适应物体的表观变化。</li>
</ul>
<p>线性更新模板的跟踪器</p>
<ul>
<li>假定表观变化率恒定。</li>
<li>缺点：<ul>
<li>实际上，模板的更新因跟踪条件的不同而有很大差异，这取决于外部因素（例如运动，模糊或背景混乱）的复杂组合。因此，简单的线性更新难以应对不断变化的更新需求，也不足以概括所有可能遇到的情况。</li>
<li>此外，这种更新方式在所有空间维度上也是恒定的，这不允许 localized partial updates。这在部分遮挡等情况下尤其有害，在这种情况下，仅模板的特定部分需要更新。</li>
<li>最后，过度依赖初始模板可能会遭受灾难性的漂移以及无法从跟踪失败中恢复过来。</li>
</ul>
</li>
</ul>
<p>本文的 UpdateNet</p>
<ul>
<li>方案：学习模板更新。学习的更新策略利用目标和图像信息，因此可以适应每种具体情况。</li>
<li>新的累积模板包含目标当前表观有效的 historical summary，因为它会使用最新信息不断更新，同时利用初始目标表观信息保证鲁棒性。</li>
</ul>
<h2 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h2><p>线性更新导致模板更集中于最近的帧而忘记历史表观。</p>
<p>为了解决这一问题，[10] [11] 提出在计算相关滤波器时，选择历史帧的子集作为训练样本。然而将多个样本保存起来导致内存和计算量增加。</p>
<blockquote>
<p>[10] Martin Danelljan, Gustav Hager, Fahad Shahbaz Khan, and Michael Felsberg. Adaptive decontamination of the training set: A unified formulation for discriminative visual tracking. In CVPR, 2016. 2<br>[11] Martin Danelljan, Andreas Robinson, Fahad Shahbaz Khan, and Michael Felsberg. Beyond correlation filters: Learning continuous convolution operators for visual tracking. In ECCV, 2016.</p>
</blockquote>
<p>ECO [7] 试图将训练样本的分布建模为混合高斯模型来缓解此问题，每个分量表示一个独特的表观。</p>
<blockquote>
<p>[7] Martin Danelljan, Goutam Bhat, F Shahbaz Khan, and Michael Felsberg. Eco: efficient convolution operators for tracking. In CVPR, 2017.</p>
</blockquote>
<p>[45] 采用长短期记忆（LSTM）通过在线跟踪过程中将先前的模板存储在存储器中来估计当前模板，然而计算量过高且算法复杂。</p>
<blockquote>
<p>[45] Tianyu Yang and Antoni B Chan. Learning dynamic memory networks for object tracking. In ECCV, 2018.</p>
</blockquote>
<p>[6] 也使用模板存储器，但是使用强化学习来选择存储的模板之一。但此方法无法从多个帧中累积信息。 </p>
<blockquote>
<p>[6] Janghoon Choi, Junseok Kwon, and Kyoung Mu Lee. Visual tracking by reinforced decision making. CoRR, abs/1702.06291, 2017.</p>
</blockquote>
<p>[33] 的元跟踪器通过预训练的方法对第一帧中目标的模型进行更好的初始化，但仍需要在线跟踪中进行线性更新。</p>
<blockquote>
<p>[33] Eunbyung Park and Alexander C Berg. Meta-tracker: Fast and robust online adaptation for visual object trackers. In ECCV, 2018.</p>
</blockquote>
<p>[46] 建议离线使用 SGD 学习 CF 跟踪器的更新系数。然而相关滤波器的求解是手工设计的，并且这些系数是固定的，在跟踪过程中不会更新。</p>
<blockquote>
<p>[46] Yingjie Yao, Xiaohe Wu, Lei Zhang, Shiguang Shan, and Wangmeng Zuo. Joint representation and truncated inference learning for correlation filter based tracking. In ECCV, 2018.</p>
</blockquote>
<p>[15] 提出通过傅立叶域中的正则化线性回归来计算相对于初始模板的变换矩阵，从而为了适应物体的表观变化。由于在估计变换时仅考虑初始模板，因此该方法忽略了在跟踪过程中观察到的历史表观变化，这可能对更平滑地适配 exemplar template 非常重要。此外，他们将变换矩阵计算为傅立叶域上的形式解，这会遇到与边界效应有关的问题。</p>
<blockquote>
<p>[15] Qing Guo, Wei Feng, Ce Zhou, Rui Huang, Liang Wan, and Song Wang. Learning dynamic siamese network for visual object tracking. In ICCV, 2017.</p>
</blockquote>
<p>我们的工作使用功能强大但易于训练的模型来更新对象模板，不仅基于第一帧，而且还使用观察到的训练数据，基于所有先前帧的累积模板，来更新目标模板。此外，我们的 UpdateNet 经过训练，可以根据观察到的训练跟踪数据学习如何有效地更新目标模板。</p>
<h2 id="Updating-the-object-template"><a href="#Updating-the-object-template" class="headerlink" title="Updating the object template"></a>Updating the object template</h2><p><img src="https://i.loli.net/2020/04/18/Y9eCiK2UqtDR6Tc.png" alt="image-20200418124944188"></p>
<h3 id="Standard-update"><a href="#Standard-update" class="headerlink" title="Standard update"></a>Standard update</h3><p>模板通过 running average 进行更新，权重随时间指数下降：</p>
<img src="https://i.loli.net/2020/04/18/ASBsKw6fMruzYUq.png" alt="image-20200418124720492" style="zoom:50%;" />

<p>$i$：frame index。</p>
<p>$T_i$：使用当前帧计算的模板。</p>
<p>$\widetilde{T}_i$：accumulated template。</p>
<p>$\gamma$：update rate，常固定为 0.01。</p>
<p>该方法具有 4 个局限性：</p>
<ol>
<li>不同情况下更新的需求是不同的。</li>
<li>在空间维度的更新是恒定的。</li>
<li>无法访问初始表观，导致无法从漂移中恢复。</li>
<li>过于简单。</li>
</ol>
<h3 id="Learning-to-update"><a href="#Learning-to-update" class="headerlink" title="Learning to update"></a>Learning to update</h3><p>通过学习 generic function $\phi$ 来更新模板：</p>
<img src="https://i.loli.net/2020/04/18/6eVvYgZwRT1IPaK.png" alt="image-20200418124657054" style="zoom:50%;" />

<p>$T_0^{GT}$：初始帧的真正模板。</p>
<h3 id="Tracking-framework-with-UpdateNet"><a href="#Tracking-framework-with-UpdateNet" class="headerlink" title="Tracking framework with UpdateNet"></a>Tracking framework with UpdateNet</h3><p>$\widetilde{T}_{i-1}$ 用于预测当前帧的目标位置。</p>
<p>将 $T_0^{GT}$， $\widetilde{T}_{i-1}$与 $T_i$ 串接起来作为 UpdateNet 的输入。</p>
<p>由于 $T_0^{GT}$ 最可靠，采用残差学习，即 UpdateNet 学习如何为当前帧修改 $T_0^{GT}$。</p>
<h3 id="Training-UpdateNet"><a href="#Training-UpdateNet" class="headerlink" title="Training UpdateNet"></a>Training UpdateNet</h3><p>最小化更新的模板与下一帧中真实模板的欧式距离：</p>
<img src="https://i.loli.net/2020/04/18/tTbvhRXM91fKrAL.png" alt="image-20200418124758354" style="zoom:50%;" />

<h4 id="Training-samples"><a href="#Training-samples" class="headerlink" title="Training samples"></a>Training samples</h4><p>训练时，对于当前帧模板 $T_i$，一种选择是使用真实位置，这意味着当前帧的预测非常准确。但是，在实际跟踪中很少遇到这种情况。这种不切实际的假设使更新偏向于期望相对于 $T_i$ 发生很小的变化，因此 UpdateNet 无法学习有用的 updating funtion。</p>
<p>因此，我们需要通过在第 $i$ 帧中使用不完美的定位来提取用于训练的 $T_i$ 样本。我们可以通过使用累积的模板 $\widetilde{T}_{i-1}$ 预测当前帧位置，来模拟这种定位不完美的情况，这与实际跟踪保持一致。</p>
<h4 id="Multi-stage-training"><a href="#Multi-stage-training" class="headerlink" title="Multi-stage training"></a>Multi-stage training</h4><p>我们可以使用由 UpdateNet 输出的 $\widetilde{T}_{i-1}$。但是这将使训练递归执行，低效而麻烦。</p>
<p>因此我们将训练分为多个阶段顺序执行。在第一阶段，在训练集上运行原始跟踪器执行标准线性更新：</p>
<img src="https://i.loli.net/2020/04/18/6VBvrpx5AydziFN.png" alt="image-20200418124837753" style="zoom:50%;" />

<p>通过这种方式来获得 accumulated templates 和每帧的预测框。</p>
<p>对于后续的每个阶段，使用上一阶段的 accumulated templates 和每帧的预测框，来训练 UpdateNet：</p>
<img src="https://i.loli.net/2020/04/18/goyKmzGTh3AWbVH.png" alt="image-20200418125408733" style="zoom:50%;" />

<h4 id="Implementation-details"><a href="#Implementation-details" class="headerlink" title="Implementation details"></a>Implementation details</h4><p>将所有模板保存在磁盘上。</p>
<p>UpdateNet 是两个卷积层。</p>
<p>采用多少历史帧？已将 $\widetilde T_{i-1}$ 保存在了磁盘上，而 $\widetilde T_{i-1}$ 表示从第一帧到当前帧的全部信息。</p>
<p>stage = 3。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/11/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><span class="page-number current">12</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/13/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">zhbli</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">123</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">zhbli</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
